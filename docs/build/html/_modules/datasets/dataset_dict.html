
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>datasets.dataset_dict &#8212; Datasets 4.4.2.dev0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/css/custom.css?v=564b2b0d" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=62d5458c"></script>
    <script src="../../_static/doctools.js?v=9bcbadda"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=fd10adb8"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/datasets/dataset_dict';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="4.4.2.dev0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
  
    <p class="title logo__title">ðŸ¤— Datasets</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../howto.html">
    How-to Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../concepts.html">
    Conceptual Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/huggingface/datasets" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://huggingface.co/datasets" title="Hugging Face" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-house fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Hugging Face</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar hide-on-wide">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../tutorials.html">
    Tutorials
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../howto.html">
    How-to Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../concepts.html">
    Conceptual Guides
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../../api/index.html">
    API Reference
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="Icon Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/huggingface/datasets" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-square-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://huggingface.co/datasets" title="Hugging Face" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-solid fa-house fa-lg" aria-hidden="true"></i>
            <span class="sr-only">Hugging Face</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../index.html" class="nav-link">Module code</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">datasets.dataset_dict</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for datasets.dataset_dict</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span><span class="w"> </span><span class="nn">contextlib</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">copy</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">fnmatch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">itertools</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">posixpath</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">re</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">collections.abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">Sequence</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">fsspec</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">fsspec.core</span><span class="w"> </span><span class="kn">import</span> <span class="n">url_to_fs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">CommitInfo</span><span class="p">,</span>
    <span class="n">CommitOperationAdd</span><span class="p">,</span>
    <span class="n">CommitOperationDelete</span><span class="p">,</span>
    <span class="n">DatasetCard</span><span class="p">,</span>
    <span class="n">DatasetCardData</span><span class="p">,</span>
    <span class="n">HfApi</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub.hf_api</span><span class="w"> </span><span class="kn">import</span> <span class="n">RepoFile</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">huggingface_hub.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">HfHubHTTPError</span><span class="p">,</span> <span class="n">RepositoryNotFoundError</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">.</span><span class="w"> </span><span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.arrow_dataset</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED</span><span class="p">,</span>
    <span class="n">Dataset</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.features</span><span class="w"> </span><span class="kn">import</span> <span class="n">Features</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.features.features</span><span class="w"> </span><span class="kn">import</span> <span class="n">FeatureType</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.info</span><span class="w"> </span><span class="kn">import</span> <span class="n">DatasetInfo</span><span class="p">,</span> <span class="n">DatasetInfosDict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.iterable_dataset</span><span class="w"> </span><span class="kn">import</span> <span class="n">IterableDataset</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.naming</span><span class="w"> </span><span class="kn">import</span> <span class="n">_split_re</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.splits</span><span class="w"> </span><span class="kn">import</span> <span class="n">NamedSplit</span><span class="p">,</span> <span class="n">Split</span><span class="p">,</span> <span class="n">SplitDict</span><span class="p">,</span> <span class="n">SplitInfo</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.table</span><span class="w"> </span><span class="kn">import</span> <span class="n">Table</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">logging</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils.doc_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_documented_by</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils.metadata</span><span class="w"> </span><span class="kn">import</span> <span class="n">MetadataConfigs</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils.py_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">asdict</span><span class="p">,</span> <span class="n">glob_pattern_to_regex</span><span class="p">,</span> <span class="n">string_to_dict</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">.utils.typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PathLike</span>


<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">class</span><span class="w"> </span><span class="nc">bind</span><span class="p">(</span><span class="n">partial</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">fn_args</span><span class="p">,</span> <span class="o">**</span><span class="n">fn_kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">fn_args</span><span class="p">,</span> <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">fn_kwargs</span><span class="p">)</span>


<div class="viewcode-block" id="DatasetDict">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.html#datasets.DatasetDict">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DatasetDict</span><span class="p">(</span><span class="nb">dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NamedSplit</span><span class="p">],</span> <span class="s2">&quot;Dataset&quot;</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A dictionary (dict of str: datasets.Dataset) with dataset transforms methods (map, filter, etc.)&quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_values_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Values in `DatasetDict` should be of type `Dataset` but got type &#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_values_features</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">items</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">())</span>
        <span class="k">for</span> <span class="n">item_a</span><span class="p">,</span> <span class="n">item_b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">items</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">items</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="k">if</span> <span class="n">item_a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span> <span class="o">!=</span> <span class="n">item_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;All datasets in `DatasetDict` should have the same features but features for &#39;</span><span class="si">{</span><span class="n">item_a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">item_b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; don&#39;t match: </span><span class="si">{</span><span class="n">item_a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">item_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__enter__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__exit__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">exc_type</span><span class="p">,</span> <span class="n">exc_val</span><span class="p">,</span> <span class="n">exc_tb</span><span class="p">):</span>
        <span class="c1"># Here `del` is used to del the pyarrow tables. This properly closes the files used for memory mapped tables</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;_data&quot;</span><span class="p">):</span>
                <span class="k">del</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_data</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;_indices&quot;</span><span class="p">):</span>
                <span class="k">del</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_indices</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dataset</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">NamedSplit</span><span class="p">))</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">available_suggested_splits</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">split</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">(</span><span class="n">Split</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">,</span> <span class="n">Split</span><span class="o">.</span><span class="n">TEST</span><span class="p">,</span> <span class="n">Split</span><span class="o">.</span><span class="n">VALIDATION</span><span class="p">)</span> <span class="k">if</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span>
            <span class="p">]</span>
            <span class="n">suggested_split</span> <span class="o">=</span> <span class="n">available_suggested_splits</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="n">available_suggested_splits</span> <span class="k">else</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
            <span class="k">raise</span> <span class="ne">KeyError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Invalid key: </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">. Please first select a split. For example: &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;`my_dataset_dictionary[&#39;</span><span class="si">{</span><span class="n">suggested_split</span><span class="si">}</span><span class="s2">&#39;][</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">]`. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Available splits: </span><span class="si">{</span><span class="nb">sorted</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Table</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The Apache Arrow tables backing each split.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.data</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">data</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">cache_files</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">dict</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The cache files containing the Apache Arrow table backing each split.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.cache_files</span>
<span class="sd">        {&#39;test&#39;: [{&#39;filename&#39;: &#39;/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-test.arrow&#39;}],</span>
<span class="sd">         &#39;train&#39;: [{&#39;filename&#39;: &#39;/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-train.arrow&#39;}],</span>
<span class="sd">         &#39;validation&#39;: [{&#39;filename&#39;: &#39;/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow&#39;}]}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cache_files</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of columns in each split of the dataset.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.num_columns</span>
<span class="sd">        {&#39;test&#39;: 2, &#39;train&#39;: 2, &#39;validation&#39;: 2}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_columns</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_rows</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of rows in each split of the dataset.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.num_rows</span>
<span class="sd">        {&#39;test&#39;: 1066, &#39;train&#39;: 8530, &#39;validation&#39;: 1066}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_rows</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">column_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Names of the columns in each split of the dataset.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.column_names</span>
<span class="sd">        {&#39;test&#39;: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;train&#39;: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;validation&#39;: [&#39;text&#39;, &#39;label&#39;]}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">shape</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Shape of each split of the dataset (number of rows, number of columns).</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.shape</span>
<span class="sd">        {&#39;test&#39;: (1066, 2), &#39;train&#39;: (8530, 2), &#39;validation&#39;: (1066, 2)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

<div class="viewcode-block" id="DatasetDict.flatten">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.flatten.html#datasets.DatasetDict.flatten">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">flatten</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Flatten the Apache Arrow Table of each split (nested features are flatten).</span>
<span class="sd">        Each column with a struct type is flattened into one column per struct field.</span>
<span class="sd">        Other columns are left unchanged.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;rajpurkar/squad&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;id&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;title&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;context&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;question&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;answers.text&#39;: List(Value(&#39;string&#39;)),</span>
<span class="sd">         &#39;answers.answer_start&#39;: List(Value(&#39;int32&#39;))}</span>
<span class="sd">        &gt;&gt;&gt; ds.flatten()</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers.text&#39;, &#39;answers.answer_start&#39;],</span>
<span class="sd">                num_rows: 87599</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;id&#39;, &#39;title&#39;, &#39;context&#39;, &#39;question&#39;, &#39;answers.text&#39;, &#39;answers.answer_start&#39;],</span>
<span class="sd">                num_rows: 10570</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.unique">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.html#datasets.DatasetDict.unique">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">unique</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return a list of the unique elements in a column for each split.</span>

<span class="sd">        This is implemented in the low-level backend and as such, very fast.</span>

<span class="sd">        Args:</span>
<span class="sd">            column (`str`):</span>
<span class="sd">                column name (list all the column names with [`~datasets.DatasetDict.column_names`])</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[`str`, `list`]: Dictionary of unique elements in the given column.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.unique(&quot;label&quot;)</span>
<span class="sd">        {&#39;test&#39;: [1, 0], &#39;train&#39;: [1, 0], &#39;validation&#39;: [1, 0]}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">column</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span></div>


<div class="viewcode-block" id="DatasetDict.cleanup_cache_files">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.cleanup_cache_files.html#datasets.DatasetDict.cleanup_cache_files">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cleanup_cache_files</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is one.</span>
<span class="sd">        Be careful when running this command that no other process is currently using other cache files.</span>

<span class="sd">        Return:</span>
<span class="sd">            `Dict` with the number of removed files for each split</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.cleanup_cache_files()</span>
<span class="sd">        {&#39;test&#39;: 0, &#39;train&#39;: 0, &#39;validation&#39;: 0}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cleanup_cache_files</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span></div>


    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">repr</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
        <span class="nb">repr</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">repr</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">M</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;DatasetDict(</span><span class="se">{{\n</span><span class="si">{</span><span class="nb">repr</span><span class="si">}</span><span class="se">\n}}</span><span class="s2">)&quot;</span>

<div class="viewcode-block" id="DatasetDict.cast">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.cast.html#datasets.DatasetDict.cast">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cast</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">:</span> <span class="n">Features</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cast the dataset to a new set of features.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            features ([`Features`]):</span>
<span class="sd">                New features to cast the dataset to.</span>
<span class="sd">                The name and order of the fields in the features must match the current column names.</span>
<span class="sd">                The type of the data must also be convertible from one type to the other.</span>
<span class="sd">                For non-trivial conversion, e.g. `string` &lt;-&gt; `ClassLabel` you should use [`~DatasetDict.map`] to update the dataset.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset, ClassLabel, Value</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;neg&#39;, &#39;pos&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        &gt;&gt;&gt; new_features = ds[&quot;train&quot;].features.copy()</span>
<span class="sd">        &gt;&gt;&gt; new_features[&#39;label&#39;] = ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;])</span>
<span class="sd">        &gt;&gt;&gt; new_features[&#39;text&#39;] = Value(&#39;large_string&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.cast(new_features)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;large_string&#39;)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.cast_column">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.cast_column.html#datasets.DatasetDict.cast_column">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">cast_column</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">feature</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cast column to feature for decoding.</span>

<span class="sd">        Args:</span>
<span class="sd">            column (`str`):</span>
<span class="sd">                Column name.</span>
<span class="sd">            feature ([`Feature`]):</span>
<span class="sd">                Target feature.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset, ClassLabel</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;neg&#39;, &#39;pos&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.cast_column(&#39;label&#39;, ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]))</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast_column</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">feature</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.remove_columns">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.remove_columns.html#datasets.DatasetDict.remove_columns">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">remove_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove one or several column(s) from each split in the dataset</span>
<span class="sd">        and the features associated to the column(s).</span>

<span class="sd">        The transformation is applied to all the splits of the dataset dictionary.</span>

<span class="sd">        You can also remove a column using [`~DatasetDict.map`] with `remove_columns` but the present method</span>
<span class="sd">        doesn&#39;t copy the data of the remaining columns and is thus faster.</span>

<span class="sd">        Args:</span>
<span class="sd">            column_names (`Union[str, list[str]]`):</span>
<span class="sd">                Name of the column(s) to remove.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]: A copy of the dataset object without the columns to remove.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.remove_columns(&quot;label&quot;)</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 8530</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">            test: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">remove_columns</span><span class="p">(</span><span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.rename_column">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.rename_column.html#datasets.DatasetDict.rename_column">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">rename_column</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">new_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rename a column in the dataset and move the features associated to the original column under the new column name.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        You can also rename a column using [`~DatasetDict.map`] with `remove_columns` but the present method:</span>
<span class="sd">            - takes care of moving the original features under the new column name.</span>
<span class="sd">            - doesn&#39;t copy the data to a new dataset and is thus much faster.</span>

<span class="sd">        Args:</span>
<span class="sd">            original_column_name (`str`):</span>
<span class="sd">                Name of the column to rename.</span>
<span class="sd">            new_column_name (`str`):</span>
<span class="sd">                New name for the column.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.rename_column(&quot;label&quot;, &quot;label_new&quot;)</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 8530</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">            test: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span>
                    <span class="n">original_column_name</span><span class="o">=</span><span class="n">original_column_name</span><span class="p">,</span>
                    <span class="n">new_column_name</span><span class="o">=</span><span class="n">new_column_name</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.rename_columns">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.rename_columns.html#datasets.DatasetDict.rename_columns">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">rename_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_mapping</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rename several columns in the dataset, and move the features associated to the original columns under</span>
<span class="sd">        the new column names.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            column_mapping (`Dict[str, str]`):</span>
<span class="sd">                A mapping of columns to rename to their new names.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]: A copy of the dataset with renamed columns.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.rename_columns({&#39;text&#39;: &#39;text_new&#39;, &#39;label&#39;: &#39;label_new&#39;})</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;text_new&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 8530</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;text_new&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">            test: Dataset({</span>
<span class="sd">                features: [&#39;text_new&#39;, &#39;label_new&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_columns</span><span class="p">(</span><span class="n">column_mapping</span><span class="o">=</span><span class="n">column_mapping</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.select_columns">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.select_columns.html#datasets.DatasetDict.select_columns">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">select_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select one or several column(s) from each split in the dataset and</span>
<span class="sd">        the features associated to the column(s).</span>

<span class="sd">        The transformation is applied to all the splits of the dataset</span>
<span class="sd">        dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            column_names (`Union[str, list[str]]`):</span>
<span class="sd">                Name of the column(s) to keep.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.select_columns(&quot;text&quot;)</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 8530</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">            test: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;],</span>
<span class="sd">                num_rows: 1066</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">select_columns</span><span class="p">(</span><span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span></div>


<div class="viewcode-block" id="DatasetDict.class_encode_column">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.class_encode_column.html#datasets.DatasetDict.class_encode_column">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">class_encode_column</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">include_nulls</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Casts the given column as [`~datasets.features.ClassLabel`] and updates the tables.</span>

<span class="sd">        Args:</span>
<span class="sd">            column (`str`):</span>
<span class="sd">                The name of the column to cast.</span>
<span class="sd">            include_nulls (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to include null values in the class labels. If `True`, the null values will be encoded as the `&quot;None&quot;` class label.</span>

<span class="sd">                &lt;Added version=&quot;1.14.2&quot;/&gt;</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;google/boolq&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;answer&#39;: Value(&#39;bool&#39;),</span>
<span class="sd">         &#39;passage&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;question&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.class_encode_column(&quot;answer&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;answer&#39;: ClassLabel(num_classes=2, names=[&#39;False&#39;, &#39;True&#39;]),</span>
<span class="sd">         &#39;passage&#39;: Value(&#39;string&#39;),</span>
<span class="sd">         &#39;question&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">class_encode_column</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">include_nulls</span><span class="o">=</span><span class="n">include_nulls</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.formatted_as">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.html#datasets.DatasetDict.formatted_as">[docs]</a>
    <span class="nd">@contextlib</span><span class="o">.</span><span class="n">contextmanager</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">formatted_as</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_all_columns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">format_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;To be used in a `with` statement. Set `__getitem__` return format (type and columns).</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            type (`str`, *optional*):</span>
<span class="sd">                Either output type selected in `[None, &#39;numpy&#39;, &#39;torch&#39;, &#39;tensorflow&#39;, &#39;jax&#39;, &#39;arrow&#39;, &#39;pandas&#39;, &#39;polars&#39;]`.</span>
<span class="sd">                `None` means `__getitem__` returns python objects (default).</span>
<span class="sd">            columns (`list[str]`, *optional*):</span>
<span class="sd">                Columns to format in the output.</span>
<span class="sd">                `None` means `__getitem__` returns all columns (default).</span>
<span class="sd">            output_all_columns (`bool`, defaults to False):</span>
<span class="sd">                Keep un-formatted columns as well in the output (as python objects).</span>
<span class="sd">            **format_kwargs (additional keyword arguments):</span>
<span class="sd">                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="n">old_format_type</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_format_type</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">old_format_kwargs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_format_kwargs</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">old_format_columns</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_format_columns</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">old_output_all_columns</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_output_all_columns</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span><span class="nb">type</span><span class="p">,</span> <span class="n">columns</span><span class="p">,</span> <span class="n">output_all_columns</span><span class="p">,</span> <span class="o">**</span><span class="n">format_kwargs</span><span class="p">)</span>
            <span class="k">yield</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
                    <span class="n">old_format_type</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">old_format_columns</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">old_output_all_columns</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="o">**</span><span class="n">old_format_kwargs</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.set_format">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.set_format.html#datasets.DatasetDict.set_format">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_format</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_all_columns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">format_kwargs</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set `__getitem__` return format (type and columns).</span>
<span class="sd">        The format is set for every dataset in the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            type (`str`, *optional*):</span>
<span class="sd">                Either output type selected in `[None, &#39;numpy&#39;, &#39;torch&#39;, &#39;tensorflow&#39;, &#39;jax&#39;, &#39;arrow&#39;, &#39;pandas&#39;, &#39;polars&#39;]`.</span>
<span class="sd">                `None` means `__getitem__` returns python objects (default).</span>
<span class="sd">            columns (`list[str]`, *optional*):</span>
<span class="sd">                Columns to format in the output.</span>
<span class="sd">                `None` means `__getitem__` returns all columns (default).</span>
<span class="sd">            output_all_columns (`bool`, defaults to False):</span>
<span class="sd">                Keep un-formatted columns as well in the output (as python objects),</span>
<span class="sd">            **format_kwargs (additional keyword arguments):</span>
<span class="sd">                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.</span>

<span class="sd">        It is possible to call `map` after calling `set_format`. Since `map` may add new columns, then the list of formatted columns</span>
<span class="sd">        gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted:</span>

<span class="sd">            `new formatted columns = (all columns - previously unformatted columns)`</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(lambda x: tokenizer(x[&quot;text&quot;], truncation=True, padding=True), batched=True)</span>
<span class="sd">        &gt;&gt;&gt; ds.set_format(type=&quot;numpy&quot;, columns=[&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;])</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].format</span>
<span class="sd">        {&#39;columns&#39;: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;format_kwargs&#39;: {},</span>
<span class="sd">         &#39;output_all_columns&#39;: False,</span>
<span class="sd">         &#39;type&#39;: &#39;numpy&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
                <span class="nb">type</span><span class="o">=</span><span class="nb">type</span><span class="p">,</span>
                <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span>
                <span class="n">output_all_columns</span><span class="o">=</span><span class="n">output_all_columns</span><span class="p">,</span>
                <span class="o">**</span><span class="n">format_kwargs</span><span class="p">,</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.reset_format">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.reset_format.html#datasets.DatasetDict.reset_format">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">reset_format</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Reset `__getitem__` return format to python objects and all columns.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Same as `self.set_format()`</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(lambda x: tokenizer(x[&quot;text&quot;], truncation=True, padding=True), batched=True)</span>
<span class="sd">        &gt;&gt;&gt; ds.set_format(type=&quot;numpy&quot;, columns=[&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;])</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].format</span>
<span class="sd">        {&#39;columns&#39;: [&#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;format_kwargs&#39;: {},</span>
<span class="sd">         &#39;output_all_columns&#39;: False,</span>
<span class="sd">         &#39;type&#39;: &#39;numpy&#39;}</span>
<span class="sd">        &gt;&gt;&gt; ds.reset_format()</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].format</span>
<span class="sd">        {&#39;columns&#39;: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],</span>
<span class="sd">         &#39;format_kwargs&#39;: {},</span>
<span class="sd">         &#39;output_all_columns&#39;: False,</span>
<span class="sd">         &#39;type&#39;: None}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">()</span></div>


<div class="viewcode-block" id="DatasetDict.set_transform">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.set_transform.html#datasets.DatasetDict.set_transform">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">set_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_all_columns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set ``__getitem__`` return format using this transform. The transform is applied on-the-fly on batches when ``__getitem__`` is called.</span>
<span class="sd">        The transform is set for every dataset in the dataset dictionary</span>
<span class="sd">        As :func:`datasets.Dataset.set_format`, this can be reset using :func:`datasets.Dataset.reset_format`</span>

<span class="sd">        Args:</span>
<span class="sd">            transform (`Callable`, optional): user-defined formatting transform, replaces the format defined by :func:`datasets.Dataset.set_format`</span>
<span class="sd">                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.</span>
<span class="sd">                This function is applied right before returning the objects in ``__getitem__``.</span>
<span class="sd">            columns (`list[str]`, optional): columns to format in the output</span>
<span class="sd">                If specified, then the input batch of the transform only contains those columns.</span>
<span class="sd">            output_all_columns (`bool`, default to False): keep un-formatted columns as well in the output (as python objects)</span>
<span class="sd">                If set to True, then the other un-formatted columns are kept with the output of the transform.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
                <span class="s2">&quot;custom&quot;</span><span class="p">,</span>
                <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span>
                <span class="n">output_all_columns</span><span class="o">=</span><span class="n">output_all_columns</span><span class="p">,</span>
                <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.with_format">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.with_format.html#datasets.DatasetDict.with_format">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">with_format</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_all_columns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">format_kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.</span>
<span class="sd">        The format `type` (for example &quot;numpy&quot;) is used to format batches when using `__getitem__`.</span>
<span class="sd">        The format is set for every dataset in the dataset dictionary.</span>

<span class="sd">        It&#39;s also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].</span>

<span class="sd">        Contrary to [`~datasets.DatasetDict.set_format`], `with_format` returns a new [`DatasetDict`] object with new [`Dataset`] objects.</span>

<span class="sd">        Args:</span>
<span class="sd">            type (`str`, *optional*):</span>
<span class="sd">                Either output type selected in `[None, &#39;numpy&#39;, &#39;torch&#39;, &#39;tensorflow&#39;, &#39;jax&#39;, &#39;arrow&#39;, &#39;pandas&#39;, &#39;polars&#39;]`.</span>
<span class="sd">                `None` means `__getitem__` returns python objects (default).</span>
<span class="sd">            columns (`list[str]`, *optional*):</span>
<span class="sd">                Columns to format in the output.</span>
<span class="sd">                `None` means `__getitem__` returns all columns (default).</span>
<span class="sd">            output_all_columns (`bool`, defaults to `False`):</span>
<span class="sd">                Keep un-formatted columns as well in the output (as python objects).</span>
<span class="sd">            **format_kwargs (additional keyword arguments):</span>
<span class="sd">                Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(lambda x: tokenizer(x[&#39;text&#39;], truncation=True, padding=True), batched=True)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].format</span>
<span class="sd">        {&#39;columns&#39;: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],</span>
<span class="sd">         &#39;format_kwargs&#39;: {},</span>
<span class="sd">         &#39;output_all_columns&#39;: False,</span>
<span class="sd">         &#39;type&#39;: None}</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.with_format(&quot;torch&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].format</span>
<span class="sd">        {&#39;columns&#39;: [&#39;text&#39;, &#39;label&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;],</span>
<span class="sd">         &#39;format_kwargs&#39;: {},</span>
<span class="sd">         &#39;output_all_columns&#39;: False,</span>
<span class="sd">         &#39;type&#39;: &#39;torch&#39;}</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;][0]</span>
<span class="sd">        {&#39;text&#39;: &#39;compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .&#39;,</span>
<span class="sd">         &#39;label&#39;: tensor(1),</span>
<span class="sd">         &#39;input_ids&#39;: tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,</span>
<span class="sd">                1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,</span>
<span class="sd">                1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0]),</span>
<span class="sd">         &#39;token_type_ids&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),</span>
<span class="sd">         &#39;attention_mask&#39;: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="sd">                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">dataset</span><span class="o">.</span><span class="n">set_format</span><span class="p">(</span>
            <span class="nb">type</span><span class="o">=</span><span class="nb">type</span><span class="p">,</span>
            <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span>
            <span class="n">output_all_columns</span><span class="o">=</span><span class="n">output_all_columns</span><span class="p">,</span>
            <span class="o">**</span><span class="n">format_kwargs</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">dataset</span></div>


<div class="viewcode-block" id="DatasetDict.with_transform">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.with_transform.html#datasets.DatasetDict.with_transform">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">with_transform</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">transform</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">],</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">output_all_columns</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.</span>
<span class="sd">        The transform is set for every dataset in the dataset dictionary</span>

<span class="sd">        As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].</span>

<span class="sd">        Contrary to [`~datasets.DatasetDict.set_transform`], `with_transform` returns a new [`DatasetDict`] object with new [`Dataset`] objects.</span>

<span class="sd">        Args:</span>
<span class="sd">            transform (`Callable`, *optional*):</span>
<span class="sd">                User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].</span>
<span class="sd">                A formatting function is a callable that takes a batch (as a dict) as input and returns a batch.</span>
<span class="sd">                This function is applied right before returning the objects in `__getitem__`.</span>
<span class="sd">            columns (`list[str]`, *optional*):</span>
<span class="sd">                Columns to format in the output.</span>
<span class="sd">                If specified, then the input batch of the transform only contains those columns.</span>
<span class="sd">            output_all_columns (`bool`, defaults to False):</span>
<span class="sd">                Keep un-formatted columns as well in the output (as python objects).</span>
<span class="sd">                If set to `True`, then the other un-formatted columns are kept with the output of the transform.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def encode(example):</span>
<span class="sd">        ...     return tokenizer(example[&#39;text&#39;], truncation=True, padding=True, return_tensors=&quot;pt&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.with_transform(encode)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;][0]</span>
<span class="sd">        {&#39;attention_mask&#39;: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="sd">         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="sd">         1, 1, 1, 1, 1, 1, 1, 1, 1]),</span>
<span class="sd">         &#39;input_ids&#39;: tensor([  101,  1103,  2067,  1110, 17348,  1106,  1129,  1103,  6880,  1432,</span>
<span class="sd">                112,   188,  1207,   107, 14255,  1389,   107,  1105,  1115,  1119,</span>
<span class="sd">                112,   188,  1280,  1106,  1294,   170, 24194,  1256,  3407,  1190,</span>
<span class="sd">                170, 11791,  5253,   188,  1732,  7200, 10947, 12606,  2895,   117,</span>
<span class="sd">                179,  7766,   118,   172, 15554,  1181,  3498,  6961,  3263,  1137,</span>
<span class="sd">                188,  1566,  7912, 14516,  6997,   119,   102]),</span>
<span class="sd">         &#39;token_type_ids&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0])}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dataset</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="n">dataset</span><span class="o">.</span><span class="n">set_transform</span><span class="p">(</span><span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span> <span class="n">output_all_columns</span><span class="o">=</span><span class="n">output_all_columns</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">dataset</span></div>


<div class="viewcode-block" id="DatasetDict.map">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.map.html#datasets.DatasetDict.map">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">with_indices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_rank</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">drop_last_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">load_from_cache_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_file_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">disable_nullable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">fn_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">desc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">try_original_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply a function to all the examples in the table (individually or in batches) and update the table.</span>
<span class="sd">        If your function returns a column that already exists, then it overwrites it.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        You can specify whether the function should be batched or not with the `batched` parameter:</span>

<span class="sd">        - If batched is `False`, then the function takes 1 example in and should return 1 example.</span>
<span class="sd">          An example is a dictionary, e.g. `{&quot;text&quot;: &quot;Hello there !&quot;}`.</span>
<span class="sd">        - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.</span>
<span class="sd">          A batch is a dictionary, e.g. a batch of 1 example is `{&quot;text&quot;: [&quot;Hello there !&quot;]}`.</span>
<span class="sd">        - If batched is `True` and `batch_size` is `n &gt; 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.</span>
<span class="sd">          Note that the last batch may have less than `n` examples.</span>
<span class="sd">          A batch is a dictionary, e.g. a batch of `n` examples is `{&quot;text&quot;: [&quot;Hello there !&quot;] * n}`.</span>

<span class="sd">        If the function is asynchronous, then `map` will run your function in parallel, with up to one thousand simultaneous calls.</span>
<span class="sd">        It is recommended to use a `asyncio.Semaphore` in your function if you want to set a maximum number of operations that can run at the same time.</span>

<span class="sd">        Args:</span>
<span class="sd">            function (`callable`): with one of the following signature:</span>
<span class="sd">                - `function(example: Dict[str, Any]) -&gt; Dict[str, Any]` if `batched=False` and `with_indices=False`</span>
<span class="sd">                - `function(example: Dict[str, Any], indices: int) -&gt; Dict[str, Any]` if `batched=False` and `with_indices=True`</span>
<span class="sd">                - `function(batch: Dict[str, list]) -&gt; Dict[str, list]` if `batched=True` and `with_indices=False`</span>
<span class="sd">                - `function(batch: Dict[str, list], indices: list[int]) -&gt; Dict[str, list]` if `batched=True` and `with_indices=True`</span>

<span class="sd">                For advanced usage, the function can also return a `pyarrow.Table`.</span>
<span class="sd">                If the function is asynchronous, then `map` will run your function in parallel.</span>
<span class="sd">                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.</span>
<span class="sd">                If no function is provided, default to identity function: `lambda x: x`.</span>
<span class="sd">            with_indices (`bool`, defaults to `False`):</span>
<span class="sd">                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.</span>
<span class="sd">            with_rank (`bool`, defaults to `False`):</span>
<span class="sd">                Provide process rank to `function`. Note that in this case the</span>
<span class="sd">                signature of `function` should be `def function(example[, idx], rank): ...`.</span>
<span class="sd">            with_split (`bool`, defaults to `False`):</span>
<span class="sd">                Provide process split to `function`. Note that in this case the</span>
<span class="sd">                signature of `function` should be `def function(example[, idx], split): ...`.</span>
<span class="sd">            input_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):</span>
<span class="sd">                The columns to be passed into `function` as</span>
<span class="sd">                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.</span>
<span class="sd">            batched (`bool`, defaults to `False`):</span>
<span class="sd">                Provide batch of examples to `function`.</span>
<span class="sd">            batch_size (`int`, *optional*, defaults to `1000`):</span>
<span class="sd">                Number of examples per batch provided to `function` if `batched=True`,</span>
<span class="sd">                `batch_size &lt;= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.</span>
<span class="sd">            drop_last_batch (`bool`, defaults to `False`):</span>
<span class="sd">                Whether a last batch smaller than the batch_size should be</span>
<span class="sd">                dropped instead of being processed by the function.</span>
<span class="sd">            remove_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):</span>
<span class="sd">                Remove a selection of columns while doing the mapping.</span>
<span class="sd">                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding</span>
<span class="sd">                columns with names in `remove_columns`, these columns will be kept.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Keep the dataset in memory instead of writing it to a cache file.</span>
<span class="sd">            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):</span>
<span class="sd">                If a cache file storing the current computation from `function`</span>
<span class="sd">                can be identified, use it instead of recomputing.</span>
<span class="sd">            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):</span>
<span class="sd">                Provide the name of a path for the cache file. It is used to store the</span>
<span class="sd">                results of the computation instead of the automatically generated cache file name.</span>
<span class="sd">                You have to provide one `cache_file_name` per dataset in the dataset dictionary.</span>
<span class="sd">            writer_batch_size (`int`, default `1000`):</span>
<span class="sd">                Number of rows per write operation for the cache file writer.</span>
<span class="sd">                This value is a good trade-off between memory usage during the processing, and processing speed.</span>
<span class="sd">                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.</span>
<span class="sd">            features (`[datasets.Features]`, *optional*, defaults to `None`):</span>
<span class="sd">                Use a specific [`Features`] to store the cache file</span>
<span class="sd">                instead of the automatically generated one.</span>
<span class="sd">            disable_nullable (`bool`, defaults to `False`):</span>
<span class="sd">                Disallow null values in the table.</span>
<span class="sd">            fn_kwargs (`Dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Keyword arguments to be passed to `function`</span>
<span class="sd">            num_proc (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                 The number of processes to use for multiprocessing.</span>
<span class="sd">                - If `None` or `0`, no multiprocessing is used and the operation runs in the main process.</span>
<span class="sd">                - If greater than `1`, one or multiple worker processes are used to process data in parallel.</span>
<span class="sd">                 Note: The function passed to `map()` must be picklable for multiprocessing to work correctly</span>
<span class="sd">                 (i.e., prefer functions defined at the top level of a module, not inside another function or class).</span>
<span class="sd">            desc (`str`, *optional*, defaults to `None`):</span>
<span class="sd">                Meaningful description to be displayed alongside with the progress bar while mapping examples.</span>
<span class="sd">            try_original_type (`Optional[bool]`, defaults to `True`):</span>
<span class="sd">                Try to keep the types of the original columns (e.g. int32 -&gt; int32).</span>
<span class="sd">                Set to False if you want to always infer new types.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; def add_prefix(example):</span>
<span class="sd">        ...     example[&quot;text&quot;] = &quot;Review: &quot; + example[&quot;text&quot;]</span>
<span class="sd">        ...     return example</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(add_prefix)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;][0:3][&quot;text&quot;]</span>
<span class="sd">        [&#39;Review: the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;,</span>
<span class="sd">         &#39;Review: the gorgeously elaborate continuation of &quot; the lord of the rings &quot; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\&#39;s expanded vision of j . r . r . tolkien\&#39;s middle-earth .&#39;,</span>
<span class="sd">         &#39;Review: effective but too-tepid biopic&#39;]</span>

<span class="sd">        # process a batch of examples</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(lambda example: tokenizer(example[&quot;text&quot;]), batched=True)</span>
<span class="sd">        # set number of processors</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(add_prefix, num_proc=4)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">cache_file_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_file_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>

        <span class="n">dataset_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">split</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">with_split</span><span class="p">:</span>
                <span class="n">function</span> <span class="o">=</span> <span class="n">bind</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">split</span><span class="p">)</span>

            <span class="n">dataset_dict</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">function</span><span class="o">=</span><span class="n">function</span><span class="p">,</span>
                <span class="n">with_indices</span><span class="o">=</span><span class="n">with_indices</span><span class="p">,</span>
                <span class="n">with_rank</span><span class="o">=</span><span class="n">with_rank</span><span class="p">,</span>
                <span class="n">input_columns</span><span class="o">=</span><span class="n">input_columns</span><span class="p">,</span>
                <span class="n">batched</span><span class="o">=</span><span class="n">batched</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">drop_last_batch</span><span class="o">=</span><span class="n">drop_last_batch</span><span class="p">,</span>
                <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">,</span>
                <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                <span class="n">load_from_cache_file</span><span class="o">=</span><span class="n">load_from_cache_file</span><span class="p">,</span>
                <span class="n">cache_file_name</span><span class="o">=</span><span class="n">cache_file_names</span><span class="p">[</span><span class="n">split</span><span class="p">],</span>
                <span class="n">writer_batch_size</span><span class="o">=</span><span class="n">writer_batch_size</span><span class="p">,</span>
                <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
                <span class="n">disable_nullable</span><span class="o">=</span><span class="n">disable_nullable</span><span class="p">,</span>
                <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">fn_kwargs</span><span class="p">,</span>
                <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="n">desc</span><span class="p">,</span>
                <span class="n">try_original_type</span><span class="o">=</span><span class="n">try_original_type</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">with_split</span><span class="p">:</span>
                <span class="n">function</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">func</span>

        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.filter">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.filter.html#datasets.DatasetDict.filter">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">filter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">with_indices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_rank</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">load_from_cache_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_file_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">fn_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">desc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply a filter function to all the elements in the table in batches</span>
<span class="sd">        and update the table so that the dataset only includes examples according to the filter function.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            function (`Callable`): Callable with one of the following signatures:</span>

<span class="sd">                - `function(example: Dict[str, Any]) -&gt; bool` if `batched=False` and `with_indices=False` and `with_rank=False`</span>
<span class="sd">                - `function(example: Dict[str, Any], *extra_args) -&gt; bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)</span>
<span class="sd">                - `function(batch: Dict[str, list]) -&gt; list[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`</span>
<span class="sd">                - `function(batch: Dict[str, list], *extra_args) -&gt; list[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)</span>

<span class="sd">                If no function is provided, defaults to an always `True` function: `lambda x: True`.</span>
<span class="sd">            with_indices (`bool`, defaults to `False`):</span>
<span class="sd">                Provide example indices to `function`. Note that in this case the</span>
<span class="sd">                signature of `function` should be `def function(example, idx[, rank]): ...`.</span>
<span class="sd">            with_rank (`bool`, defaults to `False`):</span>
<span class="sd">                Provide process rank to `function`. Note that in this case the</span>
<span class="sd">                signature of `function` should be `def function(example[, idx], rank): ...`.</span>
<span class="sd">            input_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):</span>
<span class="sd">                The columns to be passed into `function` as</span>
<span class="sd">                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.</span>
<span class="sd">            batched (`bool`, defaults to `False`):</span>
<span class="sd">                Provide batch of examples to `function`.</span>
<span class="sd">            batch_size (`int`, *optional*, defaults to `1000`):</span>
<span class="sd">                Number of examples per batch provided to `function` if `batched=True`</span>
<span class="sd">                `batch_size &lt;= 0` or `batch_size == None` then provide the full dataset as a single batch to `function`.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Keep the dataset in memory instead of writing it to a cache file.</span>
<span class="sd">            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):</span>
<span class="sd">                If a cache file storing the current computation from `function`</span>
<span class="sd">                can be identified, use it instead of recomputing.</span>
<span class="sd">            cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):</span>
<span class="sd">                Provide the name of a path for the cache file. It is used to store the</span>
<span class="sd">                results of the computation instead of the automatically generated cache file name.</span>
<span class="sd">                You have to provide one `cache_file_name` per dataset in the dataset dictionary.</span>
<span class="sd">            writer_batch_size (`int`, defaults to `1000`):</span>
<span class="sd">                Number of rows per write operation for the cache file writer.</span>
<span class="sd">                This value is a good trade-off between memory usage during the processing, and processing speed.</span>
<span class="sd">                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.</span>
<span class="sd">            fn_kwargs (`Dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Keyword arguments to be passed to `function`</span>
<span class="sd">            num_proc (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                 The number of processes to use for multiprocessing.</span>
<span class="sd">                - If `None` or `0`, no multiprocessing is used and the operation runs in the main process.</span>
<span class="sd">                - If greater than `1`, one or multiple worker processes are used to process data in parallel.</span>
<span class="sd">                 Note: The function passed to `map()` must be picklable for multiprocessing to work correctly</span>
<span class="sd">                 (i.e., prefer functions defined at the top level of a module, not inside another function or class).</span>
<span class="sd">            desc (`str`, *optional*, defaults to `None`):</span>
<span class="sd">                Meaningful description to be displayed alongside with the progress bar while filtering examples.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.filter(lambda x: x[&quot;label&quot;] == 1)</span>
<span class="sd">        DatasetDict({</span>
<span class="sd">            train: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">                num_rows: 4265</span>
<span class="sd">            })</span>
<span class="sd">            validation: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">                num_rows: 533</span>
<span class="sd">            })</span>
<span class="sd">            test: Dataset({</span>
<span class="sd">                features: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">                num_rows: 533</span>
<span class="sd">            })</span>
<span class="sd">        })</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">cache_file_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_file_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span>
                    <span class="n">function</span><span class="o">=</span><span class="n">function</span><span class="p">,</span>
                    <span class="n">with_indices</span><span class="o">=</span><span class="n">with_indices</span><span class="p">,</span>
                    <span class="n">with_rank</span><span class="o">=</span><span class="n">with_rank</span><span class="p">,</span>
                    <span class="n">input_columns</span><span class="o">=</span><span class="n">input_columns</span><span class="p">,</span>
                    <span class="n">batched</span><span class="o">=</span><span class="n">batched</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                    <span class="n">load_from_cache_file</span><span class="o">=</span><span class="n">load_from_cache_file</span><span class="p">,</span>
                    <span class="n">cache_file_name</span><span class="o">=</span><span class="n">cache_file_names</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">writer_batch_size</span><span class="o">=</span><span class="n">writer_batch_size</span><span class="p">,</span>
                    <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">fn_kwargs</span><span class="p">,</span>
                    <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
                    <span class="n">desc</span><span class="o">=</span><span class="n">desc</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.flatten_indices">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.html#datasets.DatasetDict.flatten_indices">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">flatten_indices</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">cache_file_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">disable_nullable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">new_fingerprint</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create and cache a new Dataset by flattening the indices mapping.</span>

<span class="sd">        Args:</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Keep the dataset in memory instead of writing it to a cache file.</span>
<span class="sd">            cache_file_names (`Dict[str, str]`, *optional*, default `None`):</span>
<span class="sd">                Provide the name of a path for the cache file. It is used to store the</span>
<span class="sd">                results of the computation instead of the automatically generated cache file name.</span>
<span class="sd">                You have to provide one `cache_file_name` per dataset in the dataset dictionary.</span>
<span class="sd">            writer_batch_size (`int`, defaults to `1000`):</span>
<span class="sd">                Number of rows per write operation for the cache file writer.</span>
<span class="sd">                This value is a good trade-off between memory usage during the processing, and processing speed.</span>
<span class="sd">                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.</span>
<span class="sd">            features (`Optional[datasets.Features]`, defaults to `None`):</span>
<span class="sd">                Use a specific [`Features`] to store the cache file</span>
<span class="sd">                instead of the automatically generated one.</span>
<span class="sd">            disable_nullable (`bool`, defaults to `False`):</span>
<span class="sd">                Allow null values in the table.</span>
<span class="sd">            num_proc (`int`, optional, default `None`):</span>
<span class="sd">                Max number of processes when generating cache. Already cached shards are loaded sequentially</span>
<span class="sd">            new_fingerprint (`str`, *optional*, defaults to `None`):</span>
<span class="sd">                The new fingerprint of the dataset after transform.</span>
<span class="sd">                If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">cache_file_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cache_file_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">flatten_indices</span><span class="p">(</span>
                    <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                    <span class="n">cache_file_name</span><span class="o">=</span><span class="n">cache_file_names</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">writer_batch_size</span><span class="o">=</span><span class="n">writer_batch_size</span><span class="p">,</span>
                    <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
                    <span class="n">disable_nullable</span><span class="o">=</span><span class="n">disable_nullable</span><span class="p">,</span>
                    <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
                    <span class="n">new_fingerprint</span><span class="o">=</span><span class="n">new_fingerprint</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.sort">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.sort.html#datasets.DatasetDict.sort">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">sort</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">column_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">reverse</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">bool</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="nb">bool</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">null_placement</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;at_end&quot;</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">load_from_cache_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">indices_cache_file_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new dataset sorted according to a single or multiple columns.</span>

<span class="sd">        Args:</span>
<span class="sd">            column_names (`Union[str, Sequence[str]]`):</span>
<span class="sd">                Column name(s) to sort by.</span>
<span class="sd">            reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):</span>
<span class="sd">                If `True`, sort by descending order rather than ascending. If a single bool is provided,</span>
<span class="sd">                the value is applied to the sorting of all column names. Otherwise a list of bools with the</span>
<span class="sd">                same length and order as column_names must be provided.</span>
<span class="sd">            null_placement (`str`, defaults to `at_end`):</span>
<span class="sd">                Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Keep the sorted indices in memory instead of writing it to a cache file.</span>
<span class="sd">            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):</span>
<span class="sd">                If a cache file storing the sorted indices</span>
<span class="sd">                can be identified, use it instead of recomputing.</span>
<span class="sd">            indices_cache_file_names (`[Dict[str, str]]`, *optional*, defaults to `None`):</span>
<span class="sd">                Provide the name of a path for the cache file. It is used to store the</span>
<span class="sd">                indices mapping instead of the automatically generated cache file name.</span>
<span class="sd">                You have to provide one `cache_file_name` per dataset in the dataset dictionary.</span>
<span class="sd">            writer_batch_size (`int`, defaults to `1000`):</span>
<span class="sd">                Number of rows per write operation for the cache file writer.</span>
<span class="sd">                Higher value gives smaller cache files, lower value consume less temporary memory.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&#39;cornell-movie-review-data/rotten_tomatoes&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&#39;train&#39;][&#39;label&#39;][:10]</span>
<span class="sd">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span>
<span class="sd">        &gt;&gt;&gt; sorted_ds = ds.sort(&#39;label&#39;)</span>
<span class="sd">        &gt;&gt;&gt; sorted_ds[&#39;train&#39;][&#39;label&#39;][:10]</span>
<span class="sd">        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span>
<span class="sd">        &gt;&gt;&gt; another_sorted_ds = ds.sort([&#39;label&#39;, &#39;text&#39;], reverse=[True, False])</span>
<span class="sd">        &gt;&gt;&gt; another_sorted_ds[&#39;train&#39;][&#39;label&#39;][:10]</span>
<span class="sd">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">indices_cache_file_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">indices_cache_file_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span>
                    <span class="n">column_names</span><span class="o">=</span><span class="n">column_names</span><span class="p">,</span>
                    <span class="n">reverse</span><span class="o">=</span><span class="n">reverse</span><span class="p">,</span>
                    <span class="n">null_placement</span><span class="o">=</span><span class="n">null_placement</span><span class="p">,</span>
                    <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                    <span class="n">load_from_cache_file</span><span class="o">=</span><span class="n">load_from_cache_file</span><span class="p">,</span>
                    <span class="n">indices_cache_file_name</span><span class="o">=</span><span class="n">indices_cache_file_names</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">writer_batch_size</span><span class="o">=</span><span class="n">writer_batch_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.shuffle">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.shuffle.html#datasets.DatasetDict.shuffle">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">shuffle</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seeds</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">generators</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">load_from_cache_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">indices_cache_file_names</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">writer_batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create a new Dataset where the rows are shuffled.</span>

<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Currently shuffling uses numpy random generators.</span>
<span class="sd">        You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy&#39;s default random generator (PCG64).</span>

<span class="sd">        Args:</span>
<span class="sd">            seeds (`Dict[str, int]` or `int`, *optional*):</span>
<span class="sd">                A seed to initialize the default BitGenerator if `generator=None`.</span>
<span class="sd">                If `None`, then fresh, unpredictable entropy will be pulled from the OS.</span>
<span class="sd">                If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.</span>
<span class="sd">                You can provide one `seed` per dataset in the dataset dictionary.</span>
<span class="sd">            seed (`int`, *optional*):</span>
<span class="sd">                A seed to initialize the default BitGenerator if `generator=None`. Alias for seeds (a `ValueError` is raised if both are provided).</span>
<span class="sd">            generators (`Dict[str, *optional*, np.random.Generator]`):</span>
<span class="sd">                Numpy random Generator to use to compute the permutation of the dataset rows.</span>
<span class="sd">                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).</span>
<span class="sd">                You have to provide one `generator` per dataset in the dataset dictionary.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Keep the dataset in memory instead of writing it to a cache file.</span>
<span class="sd">            load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):</span>
<span class="sd">                If a cache file storing the current computation from `function`</span>
<span class="sd">                can be identified, use it instead of recomputing.</span>
<span class="sd">            indices_cache_file_names (`Dict[str, str]`, *optional*):</span>
<span class="sd">                Provide the name of a path for the cache file. It is used to store the</span>
<span class="sd">                indices mappings instead of the automatically generated cache file name.</span>
<span class="sd">                You have to provide one `cache_file_name` per dataset in the dataset dictionary.</span>
<span class="sd">            writer_batch_size (`int`, defaults to `1000`):</span>
<span class="sd">                Number of rows per write operation for the cache file writer.</span>
<span class="sd">                This value is a good trade-off between memory usage during the processing, and processing speed.</span>
<span class="sd">                Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;][&quot;label&quot;][:10]</span>
<span class="sd">        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</span>

<span class="sd">        # set a seed</span>
<span class="sd">        &gt;&gt;&gt; shuffled_ds = ds.shuffle(seed=42)</span>
<span class="sd">        &gt;&gt;&gt; shuffled_ds[&quot;train&quot;][&quot;label&quot;][:10]</span>
<span class="sd">        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0]</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">seeds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please specify seed or seeds, but not both&quot;</span><span class="p">)</span>
        <span class="n">seeds</span> <span class="o">=</span> <span class="n">seed</span> <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">seeds</span>
        <span class="k">if</span> <span class="n">seeds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seeds</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">seeds</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="n">seeds</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seeds</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">generators</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">generators</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">indices_cache_file_names</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">indices_cache_file_names</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">generator</span><span class="o">=</span><span class="n">generators</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                    <span class="n">load_from_cache_file</span><span class="o">=</span><span class="n">load_from_cache_file</span><span class="p">,</span>
                    <span class="n">indices_cache_file_name</span><span class="o">=</span><span class="n">indices_cache_file_names</span><span class="p">[</span><span class="n">k</span><span class="p">],</span>
                    <span class="n">writer_batch_size</span><span class="o">=</span><span class="n">writer_batch_size</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.save_to_disk">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.save_to_disk.html#datasets.DatasetDict.save_to_disk">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_to_disk</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">dataset_dict_path</span><span class="p">:</span> <span class="n">PathLike</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_shards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">storage_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves a dataset dict to a filesystem using `fsspec.spec.AbstractFileSystem`.</span>

<span class="sd">        For [`Image`], [`Audio`] and [`Video`] data:</span>

<span class="sd">        All the Image(), Audio() and Video() data are stored in the arrow files.</span>
<span class="sd">        If you want to store paths or urls, please use the Value(&quot;string&quot;) type.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_dict_path (`path-like`):</span>
<span class="sd">                Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)</span>
<span class="sd">                of the dataset dict directory where the dataset dict will be saved to.</span>
<span class="sd">            max_shard_size (`int` or `str`, *optional*, defaults to `&quot;500MB&quot;`):</span>
<span class="sd">                The maximum size of the dataset shards to be saved to the filesystem. If expressed as a string, needs to be digits followed by a unit</span>
<span class="sd">                (like `&quot;50MB&quot;`).</span>
<span class="sd">            num_shards (`Dict[str, int]`, *optional*):</span>
<span class="sd">                Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.</span>
<span class="sd">                You need to provide the number of shards for each dataset in the dataset dictionary.</span>
<span class="sd">                Use a dictionary to define a different num_shards for each split.</span>

<span class="sd">                &lt;Added version=&quot;2.8.0&quot;/&gt;</span>
<span class="sd">            num_proc (`int`, *optional*, default `None`):</span>
<span class="sd">                Number of processes when downloading and generating the dataset locally.</span>
<span class="sd">                Multiprocessing is disabled by default.</span>

<span class="sd">                &lt;Added version=&quot;2.8.0&quot;/&gt;</span>
<span class="sd">            storage_options (`dict`, *optional*):</span>
<span class="sd">                Key/value pairs to be passed on to the file-system backend, if any.</span>

<span class="sd">                &lt;Added version=&quot;2.8.0&quot;/&gt;</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.save_to_disk(&quot;path/to/dataset/directory&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.save_to_disk(&quot;path/to/dataset/directory&quot;, max_shard_size=&quot;1GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.save_to_disk(&quot;path/to/dataset/directory&quot;, num_shards={&quot;train&quot;: 1024, &quot;test&quot;: 8})</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fs</span><span class="p">:</span> <span class="n">fsspec</span><span class="o">.</span><span class="n">AbstractFileSystem</span>
        <span class="n">fs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">url_to_fs</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="o">**</span><span class="p">(</span><span class="n">storage_options</span> <span class="ow">or</span> <span class="p">{}))</span>

        <span class="k">if</span> <span class="n">num_shards</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_shards</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_shards</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Please provide one `num_shards` per dataset in the dataset dictionary, e.g. {{&#39;train&#39;: 128, &#39;test&#39;: 4}}&quot;</span>
            <span class="p">)</span>

        <span class="n">fs</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">fs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span>
            <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_JSON_FILENAME</span><span class="p">),</span>
            <span class="s2">&quot;w&quot;</span><span class="p">,</span>
            <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">,</span>
        <span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">({</span><span class="s2">&quot;splits&quot;</span><span class="p">:</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="p">)},</span> <span class="n">f</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">dataset</span><span class="o">.</span><span class="n">save_to_disk</span><span class="p">(</span>
                <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span>
                <span class="n">num_shards</span><span class="o">=</span><span class="n">num_shards</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">k</span><span class="p">),</span>
                <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span><span class="p">,</span>
                <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
                <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span>
            <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.load_from_disk">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.load_from_disk.html#datasets.DatasetDict.load_from_disk">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_from_disk</span><span class="p">(</span>
        <span class="n">dataset_dict_path</span><span class="p">:</span> <span class="n">PathLike</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">storage_options</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Load a dataset that was previously saved using [`save_to_disk`] from a filesystem using `fsspec.spec.AbstractFileSystem`.</span>

<span class="sd">        Args:</span>
<span class="sd">            dataset_dict_path (`path-like`):</span>
<span class="sd">                Path (e.g. `&quot;dataset/train&quot;`) or remote URI (e.g. `&quot;s3//my-bucket/dataset/train&quot;`)</span>
<span class="sd">                of the dataset dict directory where the dataset dict will be loaded from.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `None`):</span>
<span class="sd">                Whether to copy the dataset in-memory. If `None`, the</span>
<span class="sd">                dataset will not be copied in-memory unless explicitly enabled by setting</span>
<span class="sd">                `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the</span>
<span class="sd">                [improve performance](../cache#improve-performance) section.</span>
<span class="sd">            storage_options (`dict`, *optional*):</span>
<span class="sd">                Key/value pairs to be passed on to the file-system backend, if any.</span>

<span class="sd">                &lt;Added version=&quot;2.8.0&quot;/&gt;</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; ds = load_from_disk(&#39;path/to/dataset/directory&#39;)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">fs</span><span class="p">:</span> <span class="n">fsspec</span><span class="o">.</span><span class="n">AbstractFileSystem</span>
        <span class="n">fs</span><span class="p">,</span> <span class="n">dataset_dict_path</span> <span class="o">=</span> <span class="n">url_to_fs</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="o">**</span><span class="p">(</span><span class="n">storage_options</span> <span class="ow">or</span> <span class="p">{}))</span>

        <span class="n">dataset_dict_json_path</span> <span class="o">=</span> <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_JSON_FILENAME</span><span class="p">)</span>
        <span class="n">dataset_state_json_path</span> <span class="o">=</span> <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASET_STATE_JSON_FILENAME</span><span class="p">)</span>
        <span class="n">dataset_info_path</span> <span class="o">=</span> <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASET_INFO_FILENAME</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">fs</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset_dict_json_path</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">fs</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset_info_path</span><span class="p">)</span> <span class="ow">and</span> <span class="n">fs</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">dataset_state_json_path</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;No such file: &#39;</span><span class="si">{</span><span class="n">dataset_dict_json_path</span><span class="si">}</span><span class="s2">&#39;. Expected to load a `DatasetDict` object, but got a `Dataset`. Please use either `datasets.load_from_disk` or `Dataset.load_from_disk` instead.&quot;</span>
                <span class="p">)</span>
            <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;No such file: &#39;</span><span class="si">{</span><span class="n">dataset_dict_json_path</span><span class="si">}</span><span class="s2">&#39;. Expected to load a `DatasetDict` object, but provided path is not a `DatasetDict`.&quot;</span>
            <span class="p">)</span>

        <span class="k">with</span> <span class="n">fs</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">dataset_dict_json_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">splits</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)[</span><span class="s2">&quot;splits&quot;</span><span class="p">]</span>

        <span class="n">dataset_dict</span> <span class="o">=</span> <span class="n">DatasetDict</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="n">dataset_dict_split_path</span> <span class="o">=</span> <span class="n">posixpath</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">fs</span><span class="o">.</span><span class="n">unstrip_protocol</span><span class="p">(</span><span class="n">dataset_dict_path</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
            <span class="n">dataset_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">load_from_disk</span><span class="p">(</span>
                <span class="n">dataset_dict_split_path</span><span class="p">,</span>
                <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
                <span class="n">storage_options</span><span class="o">=</span><span class="n">storage_options</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">dataset_dict</span></div>


<div class="viewcode-block" id="DatasetDict.from_csv">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.from_csv.html#datasets.DatasetDict.from_csv">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_csv</span><span class="p">(</span>
        <span class="n">path_or_paths</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PathLike</span><span class="p">],</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create [`DatasetDict`] from CSV file(s).</span>

<span class="sd">        Args:</span>
<span class="sd">            path_or_paths (`dict` of path-like):</span>
<span class="sd">                Path(s) of the CSV file(s).</span>
<span class="sd">            features ([`Features`], *optional*):</span>
<span class="sd">                Dataset features.</span>
<span class="sd">            cache_dir (str, *optional*, defaults to `&quot;~/.cache/huggingface/datasets&quot;`):</span>
<span class="sd">                Directory to cache data.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to copy the data in-memory.</span>
<span class="sd">            **kwargs (additional keyword arguments):</span>
<span class="sd">                Keyword arguments to be passed to [`pandas.read_csv`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import DatasetDict</span>
<span class="sd">        &gt;&gt;&gt; ds = DatasetDict.from_csv({&#39;train&#39;: &#39;path/to/dataset.csv&#39;})</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Dynamic import to avoid circular dependency</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io.csv</span><span class="w"> </span><span class="kn">import</span> <span class="n">CsvDatasetReader</span>

        <span class="k">return</span> <span class="n">CsvDatasetReader</span><span class="p">(</span>
            <span class="n">path_or_paths</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span></div>


<div class="viewcode-block" id="DatasetDict.from_json">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.from_json.html#datasets.DatasetDict.from_json">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_json</span><span class="p">(</span>
        <span class="n">path_or_paths</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PathLike</span><span class="p">],</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create [`DatasetDict`] from JSON Lines file(s).</span>

<span class="sd">        Args:</span>
<span class="sd">            path_or_paths (`path-like` or list of `path-like`):</span>
<span class="sd">                Path(s) of the JSON Lines file(s).</span>
<span class="sd">            features ([`Features`], *optional*):</span>
<span class="sd">                Dataset features.</span>
<span class="sd">            cache_dir (str, *optional*, defaults to `&quot;~/.cache/huggingface/datasets&quot;`):</span>
<span class="sd">                Directory to cache data.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to copy the data in-memory.</span>
<span class="sd">            **kwargs (additional keyword arguments):</span>
<span class="sd">                Keyword arguments to be passed to [`JsonConfig`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import DatasetDict</span>
<span class="sd">        &gt;&gt;&gt; ds = DatasetDict.from_json({&#39;train&#39;: &#39;path/to/dataset.json&#39;})</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Dynamic import to avoid circular dependency</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io.json</span><span class="w"> </span><span class="kn">import</span> <span class="n">JsonDatasetReader</span>

        <span class="k">return</span> <span class="n">JsonDatasetReader</span><span class="p">(</span>
            <span class="n">path_or_paths</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span></div>


<div class="viewcode-block" id="DatasetDict.from_parquet">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.from_parquet.html#datasets.DatasetDict.from_parquet">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_parquet</span><span class="p">(</span>
        <span class="n">path_or_paths</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PathLike</span><span class="p">],</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create [`DatasetDict`] from Parquet file(s).</span>

<span class="sd">        Args:</span>
<span class="sd">            path_or_paths (`dict` of path-like):</span>
<span class="sd">                Path(s) of the CSV file(s).</span>
<span class="sd">            features ([`Features`], *optional*):</span>
<span class="sd">                Dataset features.</span>
<span class="sd">            cache_dir (`str`, *optional*, defaults to `&quot;~/.cache/huggingface/datasets&quot;`):</span>
<span class="sd">                Directory to cache data.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to copy the data in-memory.</span>
<span class="sd">            columns (`list[str]`, *optional*):</span>
<span class="sd">                If not `None`, only these columns will be read from the file.</span>
<span class="sd">                A column name may be a prefix of a nested field, e.g. &#39;a&#39; will select</span>
<span class="sd">                &#39;a.b&#39;, &#39;a.c&#39;, and &#39;a.d.e&#39;.</span>
<span class="sd">            **kwargs (additional keyword arguments):</span>
<span class="sd">                Keyword arguments to be passed to [`ParquetConfig`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import DatasetDict</span>
<span class="sd">        &gt;&gt;&gt; ds = DatasetDict.from_parquet({&#39;train&#39;: &#39;path/to/dataset/parquet&#39;})</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Dynamic import to avoid circular dependency</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io.parquet</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParquetDatasetReader</span>

        <span class="k">return</span> <span class="n">ParquetDatasetReader</span><span class="p">(</span>
            <span class="n">path_or_paths</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
            <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span></div>


<div class="viewcode-block" id="DatasetDict.from_text">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.from_text.html#datasets.DatasetDict.from_text">[docs]</a>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">from_text</span><span class="p">(</span>
        <span class="n">path_or_paths</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">PathLike</span><span class="p">],</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Features</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">keep_in_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create [`DatasetDict`] from text file(s).</span>

<span class="sd">        Args:</span>
<span class="sd">            path_or_paths (`dict` of path-like):</span>
<span class="sd">                Path(s) of the text file(s).</span>
<span class="sd">            features ([`Features`], *optional*):</span>
<span class="sd">                Dataset features.</span>
<span class="sd">            cache_dir (`str`, *optional*, defaults to `&quot;~/.cache/huggingface/datasets&quot;`):</span>
<span class="sd">                Directory to cache data.</span>
<span class="sd">            keep_in_memory (`bool`, defaults to `False`):</span>
<span class="sd">                Whether to copy the data in-memory.</span>
<span class="sd">            **kwargs (additional keyword arguments):</span>
<span class="sd">                Keyword arguments to be passed to [`TextConfig`].</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`DatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import DatasetDict</span>
<span class="sd">        &gt;&gt;&gt; ds = DatasetDict.from_text({&#39;train&#39;: &#39;path/to/dataset.txt&#39;})</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Dynamic import to avoid circular dependency</span>
        <span class="kn">from</span><span class="w"> </span><span class="nn">.io.text</span><span class="w"> </span><span class="kn">import</span> <span class="n">TextDatasetReader</span>

        <span class="k">return</span> <span class="n">TextDatasetReader</span><span class="p">(</span>
            <span class="n">path_or_paths</span><span class="p">,</span>
            <span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">keep_in_memory</span><span class="o">=</span><span class="n">keep_in_memory</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span></div>


<div class="viewcode-block" id="DatasetDict.align_labels_with_mapping">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.align_labels_with_mapping.html#datasets.DatasetDict.align_labels_with_mapping">[docs]</a>
    <span class="nd">@is_documented_by</span><span class="p">(</span><span class="n">Dataset</span><span class="o">.</span><span class="n">align_labels_with_mapping</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">align_labels_with_mapping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label2id</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">label_column</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;DatasetDict&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">DatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">align_labels_with_mapping</span><span class="p">(</span><span class="n">label2id</span><span class="o">=</span><span class="n">label2id</span><span class="p">,</span> <span class="n">label_column</span><span class="o">=</span><span class="n">label_column</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="DatasetDict.push_to_hub">
<a class="viewcode-back" href="../../api/generated/datasets.DatasetDict.push_to_hub.html#datasets.DatasetDict.push_to_hub">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">push_to_hub</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">,</span>
        <span class="n">config_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">set_default</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">commit_description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">create_pr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">max_shard_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_shards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_external_files</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CommitInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pushes the [`DatasetDict`] to the hub as a Parquet dataset.</span>
<span class="sd">        The [`DatasetDict`] is pushed using HTTP requests and does not need to have neither git or git-lfs installed.</span>

<span class="sd">        Each dataset split will be pushed independently. The pushed dataset will keep the original split names.</span>

<span class="sd">        The resulting Parquet files are self-contained by default: if your dataset contains [`Image`] or [`Audio`]</span>
<span class="sd">        data, the Parquet files will store the bytes of your images or audio files.</span>
<span class="sd">        You can disable this by setting `embed_external_files` to False.</span>

<span class="sd">        Args:</span>
<span class="sd">            repo_id (`str`):</span>
<span class="sd">                The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or</span>
<span class="sd">                `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace</span>
<span class="sd">                of the logged-in user.</span>
<span class="sd">            config_name (`str`):</span>
<span class="sd">                Configuration name of a dataset. Defaults to &quot;default&quot;.</span>
<span class="sd">            set_default (`bool`, *optional*):</span>
<span class="sd">                Whether to set this configuration as the default one. Otherwise, the default configuration is the one</span>
<span class="sd">                named &quot;default&quot;.</span>
<span class="sd">            data_dir (`str`, *optional*):</span>
<span class="sd">                Directory name that will contain the uploaded data files. Defaults to the `config_name` if different</span>
<span class="sd">                from &quot;default&quot;, else &quot;data&quot;.</span>

<span class="sd">                &lt;Added version=&quot;2.17.0&quot;/&gt;</span>
<span class="sd">            commit_message (`str`, *optional*):</span>
<span class="sd">                Message to commit while pushing. Will default to `&quot;Upload dataset&quot;`.</span>
<span class="sd">            commit_description (`str`, *optional*):</span>
<span class="sd">                Description of the commit that will be created.</span>
<span class="sd">                Additionally, description of the PR if a PR is created (`create_pr` is True).</span>

<span class="sd">                &lt;Added version=&quot;2.16.0&quot;/&gt;</span>
<span class="sd">            private (`bool`, *optional*):</span>
<span class="sd">                Whether to make the repo private. If `None` (default), the repo will be public unless the</span>
<span class="sd">                organization&#39;s default is private. This value is ignored if the repo already exists.</span>
<span class="sd">            token (`str`, *optional*):</span>
<span class="sd">                An optional authentication token for the Hugging Face Hub. If no token is passed, will default</span>
<span class="sd">                to the token saved locally when logging in with `huggingface-cli login`. Will raise an error</span>
<span class="sd">                if no token is passed and the user is not logged-in.</span>
<span class="sd">            revision (`str`, *optional*):</span>
<span class="sd">                Branch to push the uploaded files to. Defaults to the `&quot;main&quot;` branch.</span>

<span class="sd">                &lt;Added version=&quot;2.15.0&quot;/&gt;</span>
<span class="sd">            create_pr (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to create a PR with the uploaded files or directly commit.</span>

<span class="sd">                &lt;Added version=&quot;2.15.0&quot;/&gt;</span>
<span class="sd">            max_shard_size (`int` or `str`, *optional*, defaults to `&quot;500MB&quot;`):</span>
<span class="sd">                The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit</span>
<span class="sd">                (like `&quot;500MB&quot;` or `&quot;1GB&quot;`).</span>
<span class="sd">            num_shards (`Dict[str, int]`, *optional*):</span>
<span class="sd">                Number of shards to write. By default, the number of shards depends on `max_shard_size`.</span>
<span class="sd">                Use a dictionary to define a different num_shards for each split.</span>

<span class="sd">                &lt;Added version=&quot;2.8.0&quot;/&gt;</span>
<span class="sd">            embed_external_files (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to embed file bytes in the shards.</span>
<span class="sd">                In particular, this will do the following before the push for the fields of type:</span>

<span class="sd">                - [`Audio`] and [`Image`] removes local path information and embed file content in the Parquet files.</span>
<span class="sd">            num_proc (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Number of processes when preparing and uploading the dataset.</span>
<span class="sd">                This is helpful if the dataset is made of many samples or media files to embed.</span>
<span class="sd">                Multiprocessing is disabled by default.</span>

<span class="sd">                &lt;Added version=&quot;4.0.0&quot;/&gt;</span>

<span class="sd">        Return:</span>
<span class="sd">            huggingface_hub.CommitInfo</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, private=True)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, max_shard_size=&quot;1GB&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, num_shards={&quot;train&quot;: 1024, &quot;test&quot;: 8})</span>
<span class="sd">        ```</span>

<span class="sd">        If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; english_dataset.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;en&quot;)</span>
<span class="sd">        &gt;&gt;&gt; french_dataset.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;fr&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # later</span>
<span class="sd">        &gt;&gt;&gt; english_dataset = load_dataset(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;en&quot;)</span>
<span class="sd">        &gt;&gt;&gt; french_dataset = load_dataset(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;fr&quot;)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_shards</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_shards</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_shards</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Please provide one `num_shards` per dataset in the dataset dictionary, e.g. {{&#39;train&#39;: 128, &#39;test&#39;: 4}}&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_features</span><span class="p">()</span>
        <span class="n">total_uploaded_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_dataset_nbytes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">info_to_dump</span><span class="p">:</span> <span class="n">DatasetInfo</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">config_name</span> <span class="o">=</span> <span class="n">config_name</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">splits</span> <span class="o">=</span> <span class="n">SplitDict</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">_split_re</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Split name should match &#39;</span><span class="si">{</span><span class="n">_split_re</span><span class="si">}</span><span class="s2">&#39; but got &#39;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

        <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">endpoint</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">HF_ENDPOINT</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">repo_info</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">id</span>
        <span class="k">except</span> <span class="n">RepositoryNotFoundError</span><span class="p">:</span>
            <span class="n">repo_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_repo</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
                <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">repo_url</span><span class="o">.</span><span class="n">repo_id</span>

        <span class="k">if</span> <span class="n">revision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">revision</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;refs/pr/&quot;</span><span class="p">):</span>
            <span class="c1"># We do not call create_branch for a PR reference: 400 Bad Request</span>
            <span class="n">api</span><span class="o">.</span><span class="n">create_branch</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">branch</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">data_dir</span><span class="p">:</span>
            <span class="n">data_dir</span> <span class="o">=</span> <span class="n">config_name</span> <span class="k">if</span> <span class="n">config_name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span> <span class="k">else</span> <span class="s2">&quot;data&quot;</span>  <span class="c1"># for backward compatibility</span>

        <span class="n">additions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pushing split </span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> to the Hub.&quot;</span><span class="p">)</span>
            <span class="c1"># The split=key needs to be removed before merging</span>
            <span class="n">split_additions</span><span class="p">,</span> <span class="n">uploaded_size</span><span class="p">,</span> <span class="n">dataset_nbytes</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">split</span><span class="p">]</span><span class="o">.</span><span class="n">_push_parquet_shards_to_hub</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span>
                <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                <span class="n">max_shard_size</span><span class="o">=</span><span class="n">max_shard_size</span><span class="p">,</span>
                <span class="n">num_shards</span><span class="o">=</span><span class="n">num_shards</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">),</span>
                <span class="n">embed_external_files</span><span class="o">=</span><span class="n">embed_external_files</span><span class="p">,</span>
                <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">additions</span> <span class="o">+=</span> <span class="n">split_additions</span>
            <span class="n">total_uploaded_size</span> <span class="o">+=</span> <span class="n">uploaded_size</span>
            <span class="n">total_dataset_nbytes</span> <span class="o">+=</span> <span class="n">dataset_nbytes</span>
            <span class="n">info_to_dump</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">SplitInfo</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="n">num_bytes</span><span class="o">=</span><span class="n">dataset_nbytes</span><span class="p">,</span> <span class="n">num_examples</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">[</span><span class="n">split</span><span class="p">]))</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">download_checksums</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">download_size</span> <span class="o">=</span> <span class="n">total_uploaded_size</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">dataset_size</span> <span class="o">=</span> <span class="n">total_dataset_nbytes</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">size_in_bytes</span> <span class="o">=</span> <span class="n">total_uploaded_size</span> <span class="o">+</span> <span class="n">total_dataset_nbytes</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_deletions_and_dataset_card</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">CommitOperationDelete</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
            <span class="n">parent_commit</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">repo_info</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">)</span><span class="o">.</span><span class="n">sha</span>

            <span class="c1"># Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)</span>
            <span class="c1"># and delete old split shards (if they exist)</span>
            <span class="n">repo_with_dataset_card</span><span class="p">,</span> <span class="n">repo_with_dataset_infos</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span>
            <span class="n">repo_splits</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># use a list to keep the order of the splits</span>
            <span class="n">deletions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">CommitOperationDelete</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">repo_files_to_add</span> <span class="o">=</span> <span class="p">[</span><span class="n">addition</span><span class="o">.</span><span class="n">path_in_repo</span> <span class="k">for</span> <span class="n">addition</span> <span class="ow">in</span> <span class="n">additions</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">repo_file</span> <span class="ow">in</span> <span class="n">api</span><span class="o">.</span><span class="n">list_repo_tree</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">repo_file</span><span class="p">,</span> <span class="n">RepoFile</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">:</span>
                    <span class="n">repo_with_dataset_card</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">:</span>
                    <span class="n">repo_with_dataset_infos</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
                    <span class="ow">and</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">repo_files_to_add</span>
                <span class="p">):</span>
                    <span class="n">deletions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CommitOperationDelete</span><span class="p">(</span><span class="n">path_in_repo</span><span class="o">=</span><span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">fnmatch</span><span class="o">.</span><span class="n">fnmatch</span><span class="p">(</span>
                    <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">,</span>
                    <span class="n">PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{split}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">),</span>
                <span class="p">):</span>
                    <span class="n">pattern</span> <span class="o">=</span> <span class="n">glob_pattern_to_regex</span><span class="p">(</span><span class="n">PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED</span><span class="p">)</span>
                    <span class="n">split_pattern_fields</span> <span class="o">=</span> <span class="n">string_to_dict</span><span class="p">(</span><span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="n">split_pattern_fields</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">repo_split</span> <span class="o">=</span> <span class="n">split_pattern_fields</span><span class="p">[</span><span class="s2">&quot;split&quot;</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">repo_split</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">repo_splits</span><span class="p">:</span>
                        <span class="n">repo_splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">repo_split</span><span class="p">)</span>

            <span class="c1"># get the info from the README to update them</span>
            <span class="k">if</span> <span class="n">repo_with_dataset_card</span><span class="p">:</span>
                <span class="n">dataset_card_path</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">hf_hub_download</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="n">DatasetCard</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">dataset_card_path</span><span class="p">))</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">dataset_card</span><span class="o">.</span><span class="n">data</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="o">.</span><span class="n">from_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="c1"># get the deprecated dataset_infos.json to update them</span>
            <span class="k">elif</span> <span class="n">repo_with_dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">DatasetCardData</span><span class="p">()</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">DatasetCardData</span><span class="p">()</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="p">()</span>
            <span class="c1"># create the metadata configs if it was uploaded with push_to_hub before metadata configs existed</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">metadata_configs</span> <span class="ow">and</span> <span class="n">repo_splits</span><span class="p">:</span>
                <span class="n">default_metadata_configs_to_dump</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;data_files&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;split&quot;</span><span class="p">:</span> <span class="n">split</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-*&quot;</span><span class="p">}</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">repo_splits</span><span class="p">]</span>
                <span class="p">}</span>
                <span class="n">MetadataConfigs</span><span class="p">({</span><span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="n">default_metadata_configs_to_dump</span><span class="p">})</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">metadata_config_to_dump</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;data_files&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;split&quot;</span><span class="p">:</span> <span class="n">split</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-*&quot;</span><span class="p">}</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()],</span>
            <span class="p">}</span>
            <span class="n">configs_to_dump</span> <span class="o">=</span> <span class="p">{</span><span class="n">config_name</span><span class="p">:</span> <span class="n">metadata_config_to_dump</span><span class="p">}</span>
            <span class="k">if</span> <span class="n">set_default</span> <span class="ow">and</span> <span class="n">config_name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">metadata_configs</span><span class="p">:</span>
                    <span class="n">current_default_config_name</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="o">.</span><span class="n">get_default_config_name</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">current_default_config_name</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;There exists a configuration named &#39;default&#39;. To set a different configuration as default, &quot;</span>
                            <span class="s2">&quot;rename the &#39;default&#39; one first.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">current_default_config_name</span><span class="p">:</span>
                        <span class="n">_</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;default&quot;</span><span class="p">)</span>
                        <span class="n">configs_to_dump</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span>
                <span class="n">metadata_config_to_dump</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># push to the deprecated dataset_infos.json</span>
            <span class="k">if</span> <span class="n">repo_with_dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_infos_path</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">hf_hub_download</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">dataset_infos_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">dataset_infos</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="n">dataset_infos</span><span class="p">[</span><span class="n">config_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="n">info_to_dump</span><span class="p">)</span>
                <span class="n">new_dataset_infos</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">dataset_infos</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_dataset_infos</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># push to README</span>
            <span class="n">DatasetInfosDict</span><span class="p">({</span><span class="n">config_name</span><span class="p">:</span> <span class="n">info_to_dump</span><span class="p">})</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">MetadataConfigs</span><span class="p">(</span><span class="n">configs_to_dump</span><span class="p">)</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">new_dataset_card</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">DatasetCard</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;---</span><span class="se">\n</span><span class="si">{</span><span class="n">dataset_card_data</span><span class="si">}</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">dataset_card</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dataset_card</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">parent_commit</span><span class="p">,</span> <span class="n">deletions</span><span class="p">,</span> <span class="n">new_dataset_card</span><span class="p">,</span> <span class="n">new_dataset_infos</span>

        <span class="n">commit_message</span> <span class="o">=</span> <span class="n">commit_message</span> <span class="k">if</span> <span class="n">commit_message</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;Upload dataset&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">additions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of files to upload is larger than </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="si">}</span><span class="s2">. Splitting the push into multiple commits.&quot;</span>
            <span class="p">)</span>
            <span class="n">num_commits</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">additions</span><span class="p">)</span> <span class="o">/</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_commits</span><span class="p">):</span>
                <span class="n">operations</span> <span class="o">=</span> <span class="n">additions</span><span class="p">[</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">retry</span><span class="p">,</span> <span class="n">sleep_time</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">30</span><span class="p">)),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                    <span class="c1"># We need to retry if another commit happens at the same time</span>
                    <span class="n">sleep_time</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">commit_info</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_commit</span><span class="p">(</span>
                            <span class="n">repo_id</span><span class="p">,</span>
                            <span class="n">operations</span><span class="o">=</span><span class="n">operations</span><span class="p">,</span>
                            <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot; (part </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">05d</span><span class="si">}</span><span class="s2">-of-</span><span class="si">{</span><span class="n">num_commits</span><span class="si">:</span><span class="s2">05d</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
                            <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                            <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                            <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">except</span> <span class="n">HfHubHTTPError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="n">err</span><span class="o">.</span><span class="n">__context__</span>
                            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="p">,</span> <span class="n">HfHubHTTPError</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">409</span>
                        <span class="p">):</span>
                            <span class="c1"># 409 is Conflict (another commit is in progress)</span>
                            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Retrying intermediate commit for </span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">config_name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">retry</span><span class="si">}</span><span class="s2">/n with status_code </span><span class="si">{</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">)&quot;</span>
                            <span class="p">)</span>
                            <span class="k">continue</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span>
                    <span class="k">break</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Commit #</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> completed&quot;</span>
                    <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot; (still </span><span class="si">{</span><span class="n">num_commits</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> to go)&quot;</span> <span class="k">if</span> <span class="n">num_commits</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
                <span class="p">)</span>
            <span class="n">last_commit_additions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">last_commit_additions</span> <span class="o">=</span> <span class="n">additions</span>

        <span class="k">for</span> <span class="n">retry</span><span class="p">,</span> <span class="n">sleep_time</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">30</span><span class="p">)),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># We need to retry if there was a commit in between in case it touched the dataset card data</span>
            <span class="n">sleep_time</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
            <span class="n">parent_commit</span><span class="p">,</span> <span class="n">deletions</span><span class="p">,</span> <span class="n">dataset_card</span><span class="p">,</span> <span class="n">dataset_infos</span> <span class="o">=</span> <span class="n">get_deletions_and_dataset_card</span><span class="p">()</span>
            <span class="n">dataset_card_additions</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_card_additions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">CommitOperationAdd</span><span class="p">(</span>
                        <span class="n">path_in_repo</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">,</span>
                        <span class="n">path_or_fileobj</span><span class="o">=</span><span class="n">dataset_infos</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">dataset_card_additions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">CommitOperationAdd</span><span class="p">(</span><span class="n">path_in_repo</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">,</span> <span class="n">path_or_fileobj</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">dataset_card</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">())</span>
            <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">commit_info</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_commit</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">operations</span><span class="o">=</span><span class="n">last_commit_additions</span> <span class="o">+</span> <span class="n">dataset_card_additions</span> <span class="o">+</span> <span class="n">deletions</span><span class="p">,</span>
                    <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
                    <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                    <span class="n">parent_commit</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="n">HfHubHTTPError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">err</span><span class="o">.</span><span class="n">__context__</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="p">,</span> <span class="n">HfHubHTTPError</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">412</span><span class="p">,</span> <span class="mi">409</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="c1"># 412 is Precondition failed (parent_commit isn&#39;t satisfied)</span>
                    <span class="c1"># 409 is Conflict (another commit is in progress)</span>
                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Retrying commit for </span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">config_name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">retry</span><span class="si">}</span><span class="s2">/n with status_code </span><span class="si">{</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span>
            <span class="k">break</span>

        <span class="k">return</span> <span class="n">commit_info</span></div>
</div>



<span class="k">class</span><span class="w"> </span><span class="nc">IterableDatasetDict</span><span class="p">(</span><span class="nb">dict</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">NamedSplit</span><span class="p">],</span> <span class="n">IterableDataset</span><span class="p">]):</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_check_values_type</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">IterableDataset</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Values in `DatasetDict` should be of type `Dataset` but got type &#39;</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span><span class="si">}</span><span class="s2">&#39;&quot;</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_check_values_features</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">items</span> <span class="o">=</span> <span class="p">[(</span><span class="n">key</span><span class="p">,</span> <span class="n">dataset</span><span class="o">.</span><span class="n">_resolve_features</span><span class="p">())</span> <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()]</span>
        <span class="k">for</span> <span class="n">item_a</span><span class="p">,</span> <span class="n">item_b</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">items</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">items</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
            <span class="k">if</span> <span class="n">item_a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span> <span class="o">!=</span> <span class="n">item_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;All datasets in `DatasetDict` should have the same features but features for &#39;</span><span class="si">{</span><span class="n">item_a</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; and &#39;</span><span class="si">{</span><span class="n">item_b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&#39; don&#39;t match: </span><span class="si">{</span><span class="n">item_a</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">item_b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">repr</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()])</span>
        <span class="nb">repr</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s2">&quot;^&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span> <span class="o">*</span> <span class="mi">4</span><span class="p">,</span> <span class="nb">repr</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">flags</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="n">M</span><span class="p">)</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;IterableDatasetDict(</span><span class="se">{{\n</span><span class="si">{</span><span class="nb">repr</span><span class="si">}</span><span class="se">\n}}</span><span class="s2">)&quot;</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">num_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Number of columns in each split of the dataset.</span>
<span class="sd">        This can contain None valies if some splits have unknown features (e.g. after a map() operation).</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.num_columns</span>
<span class="sd">        {&#39;test&#39;: 2, &#39;train&#39;: 2, &#39;validation&#39;: 2}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">num_columns</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="nd">@property</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">column_names</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Names of the columns in each split of the dataset.</span>
<span class="sd">        This can contain None valies if some splits have unknown features (e.g. after a map() operation).</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds.column_names</span>
<span class="sd">        {&#39;test&#39;: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;train&#39;: [&#39;text&#39;, &#39;label&#39;],</span>
<span class="sd">         &#39;validation&#39;: [&#39;text&#39;, &#39;label&#39;]}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">column_names</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">with_format</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="nb">type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Return a dataset with the specified format.</span>

<span class="sd">        Args:</span>

<span class="sd">            type (`str`, *optional*):</span>
<span class="sd">                Either output type selected in `[None, &#39;numpy&#39;, &#39;torch&#39;, &#39;tensorflow&#39;, &#39;jax&#39;, &#39;arrow&#39;, &#39;pandas&#39;, &#39;polars&#39;]`.</span>
<span class="sd">                `None` means it returns python objects (default).</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; from transformers import AutoTokenizer</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, split=&quot;validation&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(lambda x: tokenizer(x[&#39;text&#39;], truncation=True, padding=True), batched=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.with_format(&quot;torch&quot;)</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds))</span>
<span class="sd">        {&#39;text&#39;: &#39;compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .&#39;,</span>
<span class="sd">         &#39;label&#39;: tensor(1),</span>
<span class="sd">         &#39;input_ids&#39;: tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,</span>
<span class="sd">                1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,</span>
<span class="sd">                1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,</span>
<span class="sd">                    0,     0,     0,     0]),</span>
<span class="sd">         &#39;token_type_ids&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),</span>
<span class="sd">         &#39;attention_mask&#39;: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,</span>
<span class="sd">                1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,</span>
<span class="sd">                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">with_format</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="nb">type</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">map</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">with_indices</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">with_split</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">input_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">drop_last_batch</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">remove_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">fn_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Apply a function to all the examples in the iterable dataset (individually or in batches) and update them.</span>
<span class="sd">        If your function returns a column that already exists, then it overwrites it.</span>
<span class="sd">        The function is applied on-the-fly on the examples when iterating over the dataset.</span>
<span class="sd">        The transformation is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        You can specify whether the function should be batched or not with the `batched` parameter:</span>

<span class="sd">        - If batched is `False`, then the function takes 1 example in and should return 1 example.</span>
<span class="sd">          An example is a dictionary, e.g. `{&quot;text&quot;: &quot;Hello there !&quot;}`.</span>
<span class="sd">        - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.</span>
<span class="sd">          A batch is a dictionary, e.g. a batch of 1 example is `{&quot;text&quot;: [&quot;Hello there !&quot;]}`.</span>
<span class="sd">        - If batched is `True` and `batch_size` is `n` &gt; 1, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.</span>
<span class="sd">          Note that the last batch may have less than `n` examples.</span>
<span class="sd">          A batch is a dictionary, e.g. a batch of `n` examples is `{&quot;text&quot;: [&quot;Hello there !&quot;] * n}`.</span>

<span class="sd">        If the function is asynchronous, then `map` will run your function in parallel, with up to one thousand simultaneous calls.</span>
<span class="sd">        It is recommended to use a `asyncio.Semaphore` in your function if you want to set a maximum number of operations that can run at the same time.</span>

<span class="sd">        Args:</span>
<span class="sd">            function (`Callable`, *optional*, defaults to `None`):</span>
<span class="sd">                Function applied on-the-fly on the examples when you iterate on the dataset.</span>
<span class="sd">                It must have one of the following signatures:</span>

<span class="sd">                - `function(example: Dict[str, Any]) -&gt; Dict[str, Any]` if `batched=False` and `with_indices=False`</span>
<span class="sd">                - `function(example: Dict[str, Any], idx: int) -&gt; Dict[str, Any]` if `batched=False` and `with_indices=True`</span>
<span class="sd">                - `function(batch: Dict[str, list]) -&gt; Dict[str, list]` if `batched=True` and `with_indices=False`</span>
<span class="sd">                - `function(batch: Dict[str, list], indices: list[int]) -&gt; Dict[str, list]` if `batched=True` and `with_indices=True`</span>

<span class="sd">                For advanced usage, the function can also return a `pyarrow.Table`.</span>
<span class="sd">                If the function is asynchronous, then `map` will run your function in parallel.</span>
<span class="sd">                Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.</span>
<span class="sd">                If no function is provided, default to identity function: `lambda x: x`.</span>
<span class="sd">            with_indices (`bool`, defaults to `False`):</span>
<span class="sd">                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx[, rank]): ...`.</span>
<span class="sd">            input_columns (`[Union[str, list[str]]]`, *optional*, defaults to `None`):</span>
<span class="sd">                The columns to be passed into `function`</span>
<span class="sd">                as positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.</span>
<span class="sd">            batched (`bool`, defaults to `False`):</span>
<span class="sd">                Provide batch of examples to `function`.</span>
<span class="sd">            batch_size (`int`, *optional*, defaults to `1000`):</span>
<span class="sd">                Number of examples per batch provided to `function` if `batched=True`.</span>
<span class="sd">            drop_last_batch (`bool`, defaults to `False`):</span>
<span class="sd">                Whether a last batch smaller than the `batch_size` should be</span>
<span class="sd">                dropped instead of being processed by the function.</span>
<span class="sd">            remove_columns (`[list[str]]`, *optional*, defaults to `None`):</span>
<span class="sd">                Remove a selection of columns while doing the mapping.</span>
<span class="sd">                Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding</span>
<span class="sd">                columns with names in `remove_columns`, these columns will be kept.</span>
<span class="sd">            fn_kwargs (`Dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Keyword arguments to be passed to `function`</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; def add_prefix(example):</span>
<span class="sd">        ...     example[&quot;text&quot;] = &quot;Review: &quot; + example[&quot;text&quot;]</span>
<span class="sd">        ...     return example</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.map(add_prefix)</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds[&quot;train&quot;]))</span>
<span class="sd">        {&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &#39;Review: the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">dataset_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">split</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">with_split</span><span class="p">:</span>
                <span class="n">function</span> <span class="o">=</span> <span class="n">bind</span><span class="p">(</span><span class="n">function</span><span class="p">,</span> <span class="n">split</span><span class="p">)</span>

            <span class="n">dataset_dict</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
                <span class="n">function</span><span class="o">=</span><span class="n">function</span><span class="p">,</span>
                <span class="n">with_indices</span><span class="o">=</span><span class="n">with_indices</span><span class="p">,</span>
                <span class="n">input_columns</span><span class="o">=</span><span class="n">input_columns</span><span class="p">,</span>
                <span class="n">batched</span><span class="o">=</span><span class="n">batched</span><span class="p">,</span>
                <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                <span class="n">drop_last_batch</span><span class="o">=</span><span class="n">drop_last_batch</span><span class="p">,</span>
                <span class="n">remove_columns</span><span class="o">=</span><span class="n">remove_columns</span><span class="p">,</span>
                <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">fn_kwargs</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">with_split</span><span class="p">:</span>
                <span class="n">function</span> <span class="o">=</span> <span class="n">function</span><span class="o">.</span><span class="n">func</span>

        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span><span class="n">dataset_dict</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">filter</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">function</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">with_indices</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">input_columns</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">batched</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
        <span class="n">fn_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Apply a filter function to all the elements so that the dataset only includes examples according to the filter function.</span>
<span class="sd">        The filtering is done on-the-fly when iterating over the dataset.</span>
<span class="sd">        The filtering is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            function (`Callable`):</span>
<span class="sd">                Callable with one of the following signatures:</span>

<span class="sd">                - `function(example: Dict[str, Any]) -&gt; bool` if `with_indices=False, batched=False`</span>
<span class="sd">                - `function(example: Dict[str, Any], indices: int) -&gt; bool` if `with_indices=True, batched=False`</span>
<span class="sd">                - `function(example: Dict[str, list]) -&gt; list[bool]` if `with_indices=False, batched=True`</span>
<span class="sd">                - `function(example: Dict[str, list], indices: list[int]) -&gt; list[bool]` if `with_indices=True, batched=True`</span>

<span class="sd">                If no function is provided, defaults to an always True function: `lambda x: True`.</span>
<span class="sd">            with_indices (`bool`, defaults to `False`):</span>
<span class="sd">                Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.</span>
<span class="sd">            input_columns (`str` or `list[str]`, *optional*):</span>
<span class="sd">                The columns to be passed into `function` as</span>
<span class="sd">                positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.</span>
<span class="sd">            batched (`bool`, defaults to `False`):</span>
<span class="sd">                Provide batch of examples to `function`</span>
<span class="sd">            batch_size (`int`, *optional*, defaults to `1000`):</span>
<span class="sd">                Number of examples per batch provided to `function` if `batched=True`.</span>
<span class="sd">            fn_kwargs (`Dict`, *optional*, defaults to `None`):</span>
<span class="sd">                Keyword arguments to be passed to `function`</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.filter(lambda x: x[&quot;label&quot;] == 0)</span>
<span class="sd">        &gt;&gt;&gt; list(ds[&quot;train&quot;].take(3))</span>
<span class="sd">        [{&#39;label&#39;: 0, &#39;text&#39;: &#39;Review: simplistic , silly and tedious .&#39;},</span>
<span class="sd">         {&#39;label&#39;: 0,</span>
<span class="sd">         &#39;text&#39;: &quot;Review: it&#39;s so laddish and juvenile , only teenage boys could possibly find it funny .&quot;},</span>
<span class="sd">         {&#39;label&#39;: 0,</span>
<span class="sd">         &#39;text&#39;: &#39;Review: exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable .&#39;}]</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span>
                    <span class="n">function</span><span class="o">=</span><span class="n">function</span><span class="p">,</span>
                    <span class="n">with_indices</span><span class="o">=</span><span class="n">with_indices</span><span class="p">,</span>
                    <span class="n">input_columns</span><span class="o">=</span><span class="n">input_columns</span><span class="p">,</span>
                    <span class="n">batched</span><span class="o">=</span><span class="n">batched</span><span class="p">,</span>
                    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                    <span class="n">fn_kwargs</span><span class="o">=</span><span class="n">fn_kwargs</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">shuffle</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">generator</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">Generator</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">buffer_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Randomly shuffles the elements of this dataset.</span>
<span class="sd">        The shuffling is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        This dataset fills a buffer with buffer_size elements, then randomly samples elements from this buffer,</span>
<span class="sd">        replacing the selected elements with new elements. For perfect shuffling, a buffer size greater than or</span>
<span class="sd">        equal to the full size of the dataset is required.</span>

<span class="sd">        For instance, if your dataset contains 10,000 elements but `buffer_size` is set to 1000, then `shuffle` will</span>
<span class="sd">        initially select a random element from only the first 1000 elements in the buffer. Once an element is</span>
<span class="sd">        selected, its space in the buffer is replaced by the next (i.e. 1,001-st) element,</span>
<span class="sd">        maintaining the 1000 element buffer.</span>

<span class="sd">        If the dataset is made of several shards, it also does `shuffle` the order of the shards.</span>
<span class="sd">        However if the order has been fixed by using [`~datasets.IterableDataset.skip`] or [`~datasets.IterableDataset.take`]</span>
<span class="sd">        then the order of the shards is kept unchanged.</span>

<span class="sd">        Args:</span>
<span class="sd">            seed (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Random seed that will be used to shuffle the dataset.</span>
<span class="sd">                It is used to sample from the shuffle buffer and also to shuffle the data shards.</span>
<span class="sd">            generator (`numpy.random.Generator`, *optional*):</span>
<span class="sd">                Numpy random Generator to use to compute the permutation of the dataset rows.</span>
<span class="sd">                If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).</span>
<span class="sd">            buffer_size (`int`, defaults to `1000`):</span>
<span class="sd">                Size of the buffer.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; list(ds[&quot;train&quot;].take(3))</span>
<span class="sd">        [{&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;},</span>
<span class="sd">         {&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &#39;the gorgeously elaborate continuation of &quot; the lord of the rings &quot; trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\&#39;s expanded vision of j . r . r . tolkien\&#39;s middle-earth .&#39;},</span>
<span class="sd">         {&#39;label&#39;: 1, &#39;text&#39;: &#39;effective but too-tepid biopic&#39;}]</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.shuffle(seed=42)</span>
<span class="sd">        &gt;&gt;&gt; list(ds[&quot;train&quot;].take(3))</span>
<span class="sd">        [{&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &quot;a sports movie with action that&#39;s exciting on the field and a story you care about off it .&quot;},</span>
<span class="sd">         {&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &#39;at its best , the good girl is a refreshingly adult take on adultery . . .&#39;},</span>
<span class="sd">         {&#39;label&#39;: 1,</span>
<span class="sd">         &#39;text&#39;: &quot;sam jones became a very lucky filmmaker the day wilco got dropped from their record label , proving that one man&#39;s ruin may be another&#39;s fortune .&quot;}]</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span> <span class="n">generator</span><span class="o">=</span><span class="n">generator</span><span class="p">,</span> <span class="n">buffer_size</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">rename_column</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">original_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">new_column_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rename a column in the dataset, and move the features associated to the original column under the new column</span>
<span class="sd">        name.</span>
<span class="sd">        The renaming is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            original_column_name (`str`):</span>
<span class="sd">                Name of the column to rename.</span>
<span class="sd">            new_column_name (`str`):</span>
<span class="sd">                New name for the column.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]: A copy of the dataset with a renamed column.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.rename_column(&quot;text&quot;, &quot;movie_review&quot;)</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds[&quot;train&quot;]))</span>
<span class="sd">        {&#39;label&#39;: 1,</span>
<span class="sd">         &#39;movie_review&#39;: &#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_column</span><span class="p">(</span>
                    <span class="n">original_column_name</span><span class="o">=</span><span class="n">original_column_name</span><span class="p">,</span>
                    <span class="n">new_column_name</span><span class="o">=</span><span class="n">new_column_name</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="p">}</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">rename_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_mapping</span><span class="p">:</span> <span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Rename several columns in the dataset, and move the features associated to the original columns under</span>
<span class="sd">        the new column names.</span>
<span class="sd">        The renaming is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            column_mapping (`Dict[str, str]`):</span>
<span class="sd">                A mapping of columns to rename to their new names.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]: A copy of the dataset with renamed columns</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.rename_columns({&quot;text&quot;: &quot;movie_review&quot;, &quot;label&quot;: &quot;rating&quot;})</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds[&quot;train&quot;]))</span>
<span class="sd">        {&#39;movie_review&#39;: &#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;,</span>
<span class="sd">         &#39;rating&#39;: 1}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span>
            <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">rename_columns</span><span class="p">(</span><span class="n">column_mapping</span><span class="o">=</span><span class="n">column_mapping</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">remove_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Remove one or several column(s) in the dataset and the features associated to them.</span>
<span class="sd">        The removal is done on-the-fly on the examples when iterating over the dataset.</span>
<span class="sd">        The removal is applied to all the datasets of the dataset dictionary.</span>


<span class="sd">        Args:</span>
<span class="sd">            column_names (`Union[str, list[str]]`):</span>
<span class="sd">                Name of the column(s) to remove.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]: A copy of the dataset object without the columns to remove.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.remove_columns(&quot;label&quot;)</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds[&quot;train&quot;]))</span>
<span class="sd">        {&#39;text&#39;: &#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">remove_columns</span><span class="p">(</span><span class="n">column_names</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">select_columns</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column_names</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Select one or several column(s) in the dataset and the features</span>
<span class="sd">        associated to them. The selection is done on-the-fly on the examples</span>
<span class="sd">        when iterating over the dataset. The selection is applied to all the</span>
<span class="sd">        datasets of the dataset dictionary.</span>


<span class="sd">        Args:</span>
<span class="sd">            column_names (`Union[str, list[str]]`):</span>
<span class="sd">                Name of the column(s) to keep.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]: A copy of the dataset object with only selected columns.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.select(&quot;text&quot;)</span>
<span class="sd">        &gt;&gt;&gt; next(iter(ds[&quot;train&quot;]))</span>
<span class="sd">        {&#39;text&#39;: &#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .&#39;}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">select_columns</span><span class="p">(</span><span class="n">column_names</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cast_column</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">column</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">feature</span><span class="p">:</span> <span class="n">FeatureType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Cast column to feature for decoding.</span>
<span class="sd">        The type casting is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            column (`str`):</span>
<span class="sd">                Column name.</span>
<span class="sd">            feature ([`Feature`]):</span>
<span class="sd">                Target feature.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset, ClassLabel</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;neg&#39;, &#39;pos&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.cast_column(&#39;label&#39;, ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]))</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">(</span>
            <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast_column</span><span class="p">(</span><span class="n">column</span><span class="o">=</span><span class="n">column</span><span class="p">,</span> <span class="n">feature</span><span class="o">=</span><span class="n">feature</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">cast</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">features</span><span class="p">:</span> <span class="n">Features</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;IterableDatasetDict&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Cast the dataset to a new set of features.</span>
<span class="sd">        The type casting is applied to all the datasets of the dataset dictionary.</span>

<span class="sd">        Args:</span>
<span class="sd">            features (`Features`):</span>
<span class="sd">                New features to cast the dataset to.</span>
<span class="sd">                The name of the fields in the features must match the current column names.</span>
<span class="sd">                The type of the data must also be convertible from one type to the other.</span>
<span class="sd">                For non-trivial conversion, e.g. `string` &lt;-&gt; `ClassLabel` you should use [`map`] to update the Dataset.</span>

<span class="sd">        Returns:</span>
<span class="sd">            [`IterableDatasetDict`]: A copy of the dataset with casted features.</span>

<span class="sd">        Example:</span>

<span class="sd">        ```py</span>
<span class="sd">        &gt;&gt;&gt; from datasets import load_dataset</span>
<span class="sd">        &gt;&gt;&gt; ds = load_dataset(&quot;cornell-movie-review-data/rotten_tomatoes&quot;, streaming=True)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;neg&#39;, &#39;pos&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;string&#39;)}</span>
<span class="sd">        &gt;&gt;&gt; new_features = ds[&quot;train&quot;].features.copy()</span>
<span class="sd">        &gt;&gt;&gt; new_features[&#39;label&#39;] = ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;])</span>
<span class="sd">        &gt;&gt;&gt; new_features[&#39;text&#39;] = Value(&#39;large_string&#39;)</span>
<span class="sd">        &gt;&gt;&gt; ds = ds.cast(new_features)</span>
<span class="sd">        &gt;&gt;&gt; ds[&quot;train&quot;].features</span>
<span class="sd">        {&#39;label&#39;: ClassLabel(names=[&#39;bad&#39;, &#39;good&#39;]),</span>
<span class="sd">         &#39;text&#39;: Value(&#39;large_string&#39;)}</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">IterableDatasetDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">dataset</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">features</span><span class="o">=</span><span class="n">features</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">dataset</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">push_to_hub</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">repo_id</span><span class="p">,</span>
        <span class="n">config_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;default&quot;</span><span class="p">,</span>
        <span class="n">set_default</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">commit_message</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">commit_description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">private</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">token</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">revision</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">create_pr</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="c1"># max_shard_size: Optional[Union[int, str]] = None,  # TODO(QL): add arg</span>
        <span class="n">num_shards</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_external_files</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">num_proc</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">CommitInfo</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pushes the [`DatasetDict`] to the hub as a Parquet dataset.</span>
<span class="sd">        The [`DatasetDict`] is pushed using HTTP requests and does not need to have neither git or git-lfs installed.</span>

<span class="sd">        Each dataset split will be pushed independently. The pushed dataset will keep the original split names.</span>

<span class="sd">        The resulting Parquet files are self-contained by default: if your dataset contains [`Image`] or [`Audio`]</span>
<span class="sd">        data, the Parquet files will store the bytes of your images or audio files.</span>
<span class="sd">        You can disable this by setting `embed_external_files` to False.</span>

<span class="sd">        Args:</span>
<span class="sd">            repo_id (`str`):</span>
<span class="sd">                The ID of the repository to push to in the following format: `&lt;user&gt;/&lt;dataset_name&gt;` or</span>
<span class="sd">                `&lt;org&gt;/&lt;dataset_name&gt;`. Also accepts `&lt;dataset_name&gt;`, which will default to the namespace</span>
<span class="sd">                of the logged-in user.</span>
<span class="sd">            config_name (`str`):</span>
<span class="sd">                Configuration name of a dataset. Defaults to &quot;default&quot;.</span>
<span class="sd">            set_default (`bool`, *optional*):</span>
<span class="sd">                Whether to set this configuration as the default one. Otherwise, the default configuration is the one</span>
<span class="sd">                named &quot;default&quot;.</span>
<span class="sd">            data_dir (`str`, *optional*):</span>
<span class="sd">                Directory name that will contain the uploaded data files. Defaults to the `config_name` if different</span>
<span class="sd">                from &quot;default&quot;, else &quot;data&quot;.</span>

<span class="sd">                &lt;Added version=&quot;2.17.0&quot;/&gt;</span>
<span class="sd">            commit_message (`str`, *optional*):</span>
<span class="sd">                Message to commit while pushing. Will default to `&quot;Upload dataset&quot;`.</span>
<span class="sd">            commit_description (`str`, *optional*):</span>
<span class="sd">                Description of the commit that will be created.</span>
<span class="sd">                Additionally, description of the PR if a PR is created (`create_pr` is True).</span>

<span class="sd">                &lt;Added version=&quot;2.16.0&quot;/&gt;</span>
<span class="sd">            private (`bool`, *optional*):</span>
<span class="sd">                Whether to make the repo private. If `None` (default), the repo will be public unless the</span>
<span class="sd">                organization&#39;s default is private. This value is ignored if the repo already exists.</span>
<span class="sd">            token (`str`, *optional*):</span>
<span class="sd">                An optional authentication token for the Hugging Face Hub. If no token is passed, will default</span>
<span class="sd">                to the token saved locally when logging in with `huggingface-cli login`. Will raise an error</span>
<span class="sd">                if no token is passed and the user is not logged-in.</span>
<span class="sd">            revision (`str`, *optional*):</span>
<span class="sd">                Branch to push the uploaded files to. Defaults to the `&quot;main&quot;` branch.</span>
<span class="sd">            create_pr (`bool`, *optional*, defaults to `False`):</span>
<span class="sd">                Whether to create a PR with the uploaded files or directly commit.</span>
<span class="sd">            num_shards (`Dict[str, int]`, *optional*):</span>
<span class="sd">                Number of shards to write. Equals to this dataset&#39;s `.num_shards` by default.</span>
<span class="sd">                Use a dictionary to define a different num_shards for each split.</span>
<span class="sd">            embed_external_files (`bool`, defaults to `True`):</span>
<span class="sd">                Whether to embed file bytes in the shards.</span>
<span class="sd">                In particular, this will do the following before the push for the fields of type:</span>

<span class="sd">                - [`Audio`] and [`Image`] removes local path information and embed file content in the Parquet files.</span>
<span class="sd">            num_proc (`int`, *optional*, defaults to `None`):</span>
<span class="sd">                Number of processes when preparing and uploading the dataset.</span>
<span class="sd">                This is helpful if the dataset is made of many samples or media files to embed.</span>
<span class="sd">                Multiprocessing is disabled by default.</span>

<span class="sd">                &lt;Added version=&quot;4.0.0&quot;/&gt;</span>

<span class="sd">        Return:</span>
<span class="sd">            huggingface_hub.CommitInfo</span>

<span class="sd">        Example:</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, private=True)</span>
<span class="sd">        &gt;&gt;&gt; dataset_dict.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, num_shards={&quot;train&quot;: 1024, &quot;test&quot;: 8})</span>
<span class="sd">        ```</span>

<span class="sd">        If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):</span>

<span class="sd">        ```python</span>
<span class="sd">        &gt;&gt;&gt; english_dataset.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;en&quot;)</span>
<span class="sd">        &gt;&gt;&gt; french_dataset.push_to_hub(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;fr&quot;)</span>
<span class="sd">        &gt;&gt;&gt; # later</span>
<span class="sd">        &gt;&gt;&gt; english_dataset = load_dataset(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;en&quot;)</span>
<span class="sd">        &gt;&gt;&gt; french_dataset = load_dataset(&quot;&lt;organization&gt;/&lt;dataset_id&gt;&quot;, &quot;fr&quot;)</span>
<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">num_shards</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">num_shards</span> <span class="o">=</span> <span class="nb">dict</span><span class="o">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">num_shards</span><span class="p">,</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Please provide one `num_shards` per dataset in the dataset dictionary, e.g. {{&#39;train&#39;: 128, &#39;test&#39;: 4}}&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_type</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_values_features</span><span class="p">()</span>
        <span class="n">total_uploaded_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">total_dataset_nbytes</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">info_to_dump</span><span class="p">:</span> <span class="n">DatasetInfo</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span><span class="o">.</span><span class="n">info</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">config_name</span> <span class="o">=</span> <span class="n">config_name</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">splits</span> <span class="o">=</span> <span class="n">SplitDict</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">re</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">_split_re</span><span class="p">,</span> <span class="n">split</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Split name should match &#39;</span><span class="si">{</span><span class="n">_split_re</span><span class="si">}</span><span class="s2">&#39; but got &#39;</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">&#39;.&quot;</span><span class="p">)</span>

        <span class="n">api</span> <span class="o">=</span> <span class="n">HfApi</span><span class="p">(</span><span class="n">endpoint</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">HF_ENDPOINT</span><span class="p">,</span> <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">repo_info</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">id</span>
        <span class="k">except</span> <span class="n">RepositoryNotFoundError</span><span class="p">:</span>
            <span class="n">repo_url</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_repo</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">private</span><span class="o">=</span><span class="n">private</span><span class="p">,</span>
                <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">repo_id</span> <span class="o">=</span> <span class="n">repo_url</span><span class="o">.</span><span class="n">repo_id</span>

        <span class="k">if</span> <span class="n">revision</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">revision</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;refs/pr/&quot;</span><span class="p">):</span>
            <span class="c1"># We do not call create_branch for a PR reference: 400 Bad Request</span>
            <span class="n">api</span><span class="o">.</span><span class="n">create_branch</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">branch</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">data_dir</span><span class="p">:</span>
            <span class="n">data_dir</span> <span class="o">=</span> <span class="n">config_name</span> <span class="k">if</span> <span class="n">config_name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span> <span class="k">else</span> <span class="s2">&quot;data&quot;</span>  <span class="c1"># for backward compatibility</span>

        <span class="n">additions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Pushing split </span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2"> to the Hub.&quot;</span><span class="p">)</span>
            <span class="c1"># The split=key needs to be removed before merging</span>
            <span class="n">split_additions</span><span class="p">,</span> <span class="n">uploaded_size</span><span class="p">,</span> <span class="n">dataset_nbytes</span><span class="p">,</span> <span class="n">num_examples</span> <span class="o">=</span> <span class="bp">self</span><span class="p">[</span><span class="n">split</span><span class="p">]</span><span class="o">.</span><span class="n">_push_parquet_shards_to_hub</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="p">,</span>
                <span class="n">data_dir</span><span class="o">=</span><span class="n">data_dir</span><span class="p">,</span>
                <span class="n">split</span><span class="o">=</span><span class="n">split</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                <span class="c1"># max_shard_size=max_shard_size,  # TODO(QL): add arg</span>
                <span class="n">num_shards</span><span class="o">=</span><span class="n">num_shards</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">split</span><span class="p">),</span>
                <span class="n">embed_external_files</span><span class="o">=</span><span class="n">embed_external_files</span><span class="p">,</span>
                <span class="n">num_proc</span><span class="o">=</span><span class="n">num_proc</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="n">additions</span> <span class="o">+=</span> <span class="n">split_additions</span>
            <span class="n">total_uploaded_size</span> <span class="o">+=</span> <span class="n">uploaded_size</span>
            <span class="n">total_dataset_nbytes</span> <span class="o">+=</span> <span class="n">dataset_nbytes</span>
            <span class="n">info_to_dump</span><span class="o">.</span><span class="n">splits</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">SplitInfo</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">split</span><span class="p">),</span> <span class="n">num_bytes</span><span class="o">=</span><span class="n">dataset_nbytes</span><span class="p">,</span> <span class="n">num_examples</span><span class="o">=</span><span class="n">num_examples</span><span class="p">)</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">download_checksums</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">download_size</span> <span class="o">=</span> <span class="n">total_uploaded_size</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">dataset_size</span> <span class="o">=</span> <span class="n">total_dataset_nbytes</span>
        <span class="n">info_to_dump</span><span class="o">.</span><span class="n">size_in_bytes</span> <span class="o">=</span> <span class="n">total_uploaded_size</span> <span class="o">+</span> <span class="n">total_dataset_nbytes</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">get_deletions_and_dataset_card</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">list</span><span class="p">[</span><span class="n">CommitOperationDelete</span><span class="p">],</span> <span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]]:</span>
            <span class="n">parent_commit</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">repo_info</span><span class="p">(</span><span class="n">repo_id</span><span class="p">,</span> <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">)</span><span class="o">.</span><span class="n">sha</span>

            <span class="c1"># Check if the repo already has a README.md and/or a dataset_infos.json to update them with the new split info (size and pattern)</span>
            <span class="c1"># and delete old split shards (if they exist)</span>
            <span class="n">repo_with_dataset_card</span><span class="p">,</span> <span class="n">repo_with_dataset_infos</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">False</span>
            <span class="n">repo_splits</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># use a list to keep the order of the splits</span>
            <span class="n">deletions</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">CommitOperationDelete</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">repo_files_to_add</span> <span class="o">=</span> <span class="p">[</span><span class="n">addition</span><span class="o">.</span><span class="n">path_in_repo</span> <span class="k">for</span> <span class="n">addition</span> <span class="ow">in</span> <span class="n">additions</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">repo_file</span> <span class="ow">in</span> <span class="n">api</span><span class="o">.</span><span class="n">list_repo_tree</span><span class="p">(</span>
                <span class="n">repo_id</span><span class="o">=</span><span class="n">repo_id</span><span class="p">,</span>
                <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                <span class="n">token</span><span class="o">=</span><span class="n">token</span><span class="p">,</span>
                <span class="n">recursive</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">):</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">repo_file</span><span class="p">,</span> <span class="n">RepoFile</span><span class="p">):</span>
                    <span class="k">continue</span>
                <span class="k">if</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">:</span>
                    <span class="n">repo_with_dataset_card</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="o">==</span> <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">:</span>
                    <span class="n">repo_with_dataset_infos</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="k">elif</span> <span class="p">(</span>
                    <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="nb">tuple</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-&quot;</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
                    <span class="ow">and</span> <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">repo_files_to_add</span>
                <span class="p">):</span>
                    <span class="n">deletions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">CommitOperationDelete</span><span class="p">(</span><span class="n">path_in_repo</span><span class="o">=</span><span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">))</span>
                <span class="k">elif</span> <span class="n">fnmatch</span><span class="o">.</span><span class="n">fnmatch</span><span class="p">(</span>
                    <span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">,</span>
                    <span class="n">PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{split}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="s2">&quot;*&quot;</span><span class="p">),</span>
                <span class="p">):</span>
                    <span class="n">pattern</span> <span class="o">=</span> <span class="n">glob_pattern_to_regex</span><span class="p">(</span><span class="n">PUSH_TO_HUB_WITHOUT_METADATA_CONFIGS_SPLIT_PATTERN_SHARDED</span><span class="p">)</span>
                    <span class="n">split_pattern_fields</span> <span class="o">=</span> <span class="n">string_to_dict</span><span class="p">(</span><span class="n">repo_file</span><span class="o">.</span><span class="n">rfilename</span><span class="p">,</span> <span class="n">pattern</span><span class="p">)</span>
                    <span class="k">assert</span> <span class="n">split_pattern_fields</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">repo_split</span> <span class="o">=</span> <span class="n">split_pattern_fields</span><span class="p">[</span><span class="s2">&quot;split&quot;</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">repo_split</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">repo_splits</span><span class="p">:</span>
                        <span class="n">repo_splits</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">repo_split</span><span class="p">)</span>

            <span class="c1"># get the info from the README to update them</span>
            <span class="k">if</span> <span class="n">repo_with_dataset_card</span><span class="p">:</span>
                <span class="n">dataset_card_path</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">hf_hub_download</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="n">DatasetCard</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Path</span><span class="p">(</span><span class="n">dataset_card_path</span><span class="p">))</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">dataset_card</span><span class="o">.</span><span class="n">data</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="o">.</span><span class="n">from_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="c1"># get the deprecated dataset_infos.json to update them</span>
            <span class="k">elif</span> <span class="n">repo_with_dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">DatasetCardData</span><span class="p">()</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="p">()</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dataset_card</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="n">dataset_card_data</span> <span class="o">=</span> <span class="n">DatasetCardData</span><span class="p">()</span>
                <span class="n">metadata_configs</span> <span class="o">=</span> <span class="n">MetadataConfigs</span><span class="p">()</span>
            <span class="c1"># create the metadata configs if it was uploaded with push_to_hub before metadata configs existed</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">metadata_configs</span> <span class="ow">and</span> <span class="n">repo_splits</span><span class="p">:</span>
                <span class="n">default_metadata_configs_to_dump</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="s2">&quot;data_files&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;split&quot;</span><span class="p">:</span> <span class="n">split</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;data/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-*&quot;</span><span class="p">}</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">repo_splits</span><span class="p">]</span>
                <span class="p">}</span>
                <span class="n">MetadataConfigs</span><span class="p">({</span><span class="s2">&quot;default&quot;</span><span class="p">:</span> <span class="n">default_metadata_configs_to_dump</span><span class="p">})</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">metadata_config_to_dump</span> <span class="o">=</span> <span class="p">{</span>
                <span class="s2">&quot;data_files&quot;</span><span class="p">:</span> <span class="p">[{</span><span class="s2">&quot;split&quot;</span><span class="p">:</span> <span class="n">split</span><span class="p">,</span> <span class="s2">&quot;path&quot;</span><span class="p">:</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">data_dir</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">split</span><span class="si">}</span><span class="s2">-*&quot;</span><span class="p">}</span> <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">keys</span><span class="p">()],</span>
            <span class="p">}</span>
            <span class="n">configs_to_dump</span> <span class="o">=</span> <span class="p">{</span><span class="n">config_name</span><span class="p">:</span> <span class="n">metadata_config_to_dump</span><span class="p">}</span>
            <span class="k">if</span> <span class="n">set_default</span> <span class="ow">and</span> <span class="n">config_name</span> <span class="o">!=</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">metadata_configs</span><span class="p">:</span>
                    <span class="n">current_default_config_name</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="o">.</span><span class="n">get_default_config_name</span><span class="p">()</span>
                    <span class="k">if</span> <span class="n">current_default_config_name</span> <span class="o">==</span> <span class="s2">&quot;default&quot;</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;There exists a configuration named &#39;default&#39;. To set a different configuration as default, &quot;</span>
                            <span class="s2">&quot;rename the &#39;default&#39; one first.&quot;</span>
                        <span class="p">)</span>
                    <span class="k">if</span> <span class="n">current_default_config_name</span><span class="p">:</span>
                        <span class="n">_</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;default&quot;</span><span class="p">)</span>
                        <span class="n">configs_to_dump</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">metadata_configs</span><span class="p">[</span><span class="n">current_default_config_name</span><span class="p">]</span>
                <span class="n">metadata_config_to_dump</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="c1"># push to the deprecated dataset_infos.json</span>
            <span class="k">if</span> <span class="n">repo_with_dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_infos_path</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">hf_hub_download</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">dataset_infos_path</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                    <span class="n">dataset_infos</span><span class="p">:</span> <span class="nb">dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="n">dataset_infos</span><span class="p">[</span><span class="n">config_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">asdict</span><span class="p">(</span><span class="n">info_to_dump</span><span class="p">)</span>
                <span class="n">new_dataset_infos</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">dataset_infos</span><span class="p">,</span> <span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">new_dataset_infos</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="c1"># push to README</span>
            <span class="n">DatasetInfosDict</span><span class="p">({</span><span class="n">config_name</span><span class="p">:</span> <span class="n">info_to_dump</span><span class="p">})</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">MetadataConfigs</span><span class="p">(</span><span class="n">configs_to_dump</span><span class="p">)</span><span class="o">.</span><span class="n">to_dataset_card_data</span><span class="p">(</span><span class="n">dataset_card_data</span><span class="p">)</span>
            <span class="n">new_dataset_card</span> <span class="o">=</span> <span class="p">(</span>
                <span class="n">DatasetCard</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;---</span><span class="se">\n</span><span class="si">{</span><span class="n">dataset_card_data</span><span class="si">}</span><span class="se">\n</span><span class="s2">---</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span> <span class="k">if</span> <span class="n">dataset_card</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">dataset_card</span>
            <span class="p">)</span>
            <span class="k">return</span> <span class="n">parent_commit</span><span class="p">,</span> <span class="n">deletions</span><span class="p">,</span> <span class="n">new_dataset_card</span><span class="p">,</span> <span class="n">new_dataset_infos</span>

        <span class="n">commit_message</span> <span class="o">=</span> <span class="n">commit_message</span> <span class="k">if</span> <span class="n">commit_message</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;Upload dataset&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">additions</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Number of files to upload is larger than </span><span class="si">{</span><span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="si">}</span><span class="s2">. Splitting the push into multiple commits.&quot;</span>
            <span class="p">)</span>
            <span class="n">num_commits</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">additions</span><span class="p">)</span> <span class="o">/</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_commits</span><span class="p">):</span>
                <span class="n">operations</span> <span class="o">=</span> <span class="n">additions</span><span class="p">[</span>
                    <span class="n">i</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span> <span class="p">:</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">UPLOADS_MAX_NUMBER_PER_COMMIT</span>
                <span class="p">]</span>
                <span class="k">for</span> <span class="n">retry</span><span class="p">,</span> <span class="n">sleep_time</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">30</span><span class="p">)),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
                    <span class="c1"># We need to retry if another commit happens at the same time</span>
                    <span class="n">sleep_time</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="n">commit_info</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_commit</span><span class="p">(</span>
                            <span class="n">repo_id</span><span class="p">,</span>
                            <span class="n">operations</span><span class="o">=</span><span class="n">operations</span><span class="p">,</span>
                            <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span> <span class="o">+</span> <span class="sa">f</span><span class="s2">&quot; (part </span><span class="si">{</span><span class="n">i</span><span class="si">:</span><span class="s2">05d</span><span class="si">}</span><span class="s2">-of-</span><span class="si">{</span><span class="n">num_commits</span><span class="si">:</span><span class="s2">05d</span><span class="si">}</span><span class="s2">)&quot;</span><span class="p">,</span>
                            <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                            <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                            <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                            <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">except</span> <span class="n">HfHubHTTPError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                        <span class="k">if</span> <span class="p">(</span>
                            <span class="n">err</span><span class="o">.</span><span class="n">__context__</span>
                            <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="p">,</span> <span class="n">HfHubHTTPError</span><span class="p">)</span>
                            <span class="ow">and</span> <span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">409</span>
                        <span class="p">):</span>
                            <span class="c1"># 409 is Conflict (another commit is in progress)</span>
                            <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                                <span class="sa">f</span><span class="s2">&quot;Retrying intermediate commit for </span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">config_name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">retry</span><span class="si">}</span><span class="s2">/n with status_code </span><span class="si">{</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">)&quot;</span>
                            <span class="p">)</span>
                            <span class="k">continue</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="k">raise</span>
                    <span class="k">break</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Commit #</span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> completed&quot;</span>
                    <span class="o">+</span> <span class="p">(</span><span class="sa">f</span><span class="s2">&quot; (still </span><span class="si">{</span><span class="n">num_commits</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> to go)&quot;</span> <span class="k">if</span> <span class="n">num_commits</span> <span class="o">-</span> <span class="n">i</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">else</span> <span class="s2">&quot;&quot;</span><span class="p">)</span>
                    <span class="o">+</span> <span class="s2">&quot;.&quot;</span>
                <span class="p">)</span>
            <span class="n">last_commit_additions</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">last_commit_additions</span> <span class="o">=</span> <span class="n">additions</span>

        <span class="k">for</span> <span class="n">retry</span><span class="p">,</span> <span class="n">sleep_time</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">30</span><span class="p">)),</span> <span class="n">start</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
            <span class="c1"># We need to retry if there was a commit in between in case it touched the dataset card data</span>
            <span class="n">sleep_time</span> <span class="o">*=</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
            <span class="n">parent_commit</span><span class="p">,</span> <span class="n">deletions</span><span class="p">,</span> <span class="n">dataset_card</span><span class="p">,</span> <span class="n">dataset_infos</span> <span class="o">=</span> <span class="n">get_deletions_and_dataset_card</span><span class="p">()</span>
            <span class="n">dataset_card_additions</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">if</span> <span class="n">dataset_infos</span><span class="p">:</span>
                <span class="n">dataset_card_additions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                    <span class="n">CommitOperationAdd</span><span class="p">(</span>
                        <span class="n">path_in_repo</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">DATASETDICT_INFOS_FILENAME</span><span class="p">,</span>
                        <span class="n">path_or_fileobj</span><span class="o">=</span><span class="n">dataset_infos</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">),</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="n">dataset_card_additions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
                <span class="n">CommitOperationAdd</span><span class="p">(</span><span class="n">path_in_repo</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">REPOCARD_FILENAME</span><span class="p">,</span> <span class="n">path_or_fileobj</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">dataset_card</span><span class="p">)</span><span class="o">.</span><span class="n">encode</span><span class="p">())</span>
            <span class="p">)</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">commit_info</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">create_commit</span><span class="p">(</span>
                    <span class="n">repo_id</span><span class="p">,</span>
                    <span class="n">operations</span><span class="o">=</span><span class="n">last_commit_additions</span> <span class="o">+</span> <span class="n">dataset_card_additions</span> <span class="o">+</span> <span class="n">deletions</span><span class="p">,</span>
                    <span class="n">commit_message</span><span class="o">=</span><span class="n">commit_message</span><span class="p">,</span>
                    <span class="n">commit_description</span><span class="o">=</span><span class="n">commit_description</span><span class="p">,</span>
                    <span class="n">repo_type</span><span class="o">=</span><span class="s2">&quot;dataset&quot;</span><span class="p">,</span>
                    <span class="n">revision</span><span class="o">=</span><span class="n">revision</span><span class="p">,</span>
                    <span class="n">create_pr</span><span class="o">=</span><span class="n">create_pr</span><span class="p">,</span>
                    <span class="n">parent_commit</span><span class="o">=</span><span class="n">parent_commit</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">except</span> <span class="n">HfHubHTTPError</span> <span class="k">as</span> <span class="n">err</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">err</span><span class="o">.</span><span class="n">__context__</span>
                    <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="p">,</span> <span class="n">HfHubHTTPError</span><span class="p">)</span>
                    <span class="ow">and</span> <span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">412</span><span class="p">,</span> <span class="mi">409</span><span class="p">)</span>
                <span class="p">):</span>
                    <span class="c1"># 412 is Precondition failed (parent_commit isn&#39;t satisfied)</span>
                    <span class="c1"># 409 is Conflict (another commit is in progress)</span>
                    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="n">sleep_time</span><span class="p">)</span>
                    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Retrying commit for </span><span class="si">{</span><span class="n">repo_id</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="n">config_name</span><span class="si">}</span><span class="s2"> (</span><span class="si">{</span><span class="n">retry</span><span class="si">}</span><span class="s2">/n with status_code </span><span class="si">{</span><span class="n">err</span><span class="o">.</span><span class="n">__context__</span><span class="o">.</span><span class="n">response</span><span class="o">.</span><span class="n">status_code</span><span class="si">}</span><span class="s2">)&quot;</span>
                    <span class="p">)</span>
                    <span class="k">continue</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span>
            <span class="k">break</span>

        <span class="k">return</span> <span class="n">commit_info</span>
</pre></div>

                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      Â© Copyright 2025, HuggingFace Inc..
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>